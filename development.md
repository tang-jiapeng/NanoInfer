# NanoInfer 开发计划

> Generated By Cladue Opus 4.6

本文档记录 NanoInfer 下一阶段的开发方向，包括代码审查发现的改进点、新功能规划和性能优化路线。

---

## 一、代码审查：当前发现的问题和改进点

### 1.1 Chunked Prefill Kernel — 每层 7 次 cudaMallocAsync

**问题**：`prefill_attention_kernel` 中每次调用（即每层、每 chunk）都执行 7 次 `cudaMallocAsync` + `cudaFreeAsync`（k_gathered, v_gathered, q_reshaped, k_reshaped, v_reshaped, scores, out_reshaped）。对 22 层模型，单个 chunk 就是 154 次 async alloc/free。

**改进方案**：
1. **预分配工作空间**：在 Engine 或 Model 初始化时，根据 `max(chunk_size × context_budget)` 预分配一块固定大小的 workspace buffer，kernel 内部用偏移量取子区间，避免运行时动态分配。
2. **双缓冲（Ping-Pong）**：如果需要支持多个 prefill 请求并行（CUDA multi-stream），可分配两组 workspace 交替使用。
3. **预计算最大尺寸**：`max_scores_size = num_heads × chunk_size × max_seq_len × sizeof(float)`，初始化时一次性 malloc。

### 1.2 Chunked Prefill — Gather 全量 K/V 效率

**问题**：每层 prefill 时都从 Paged Cache 中 Gather `[0, context_len)` 的全部 K/V 到连续 buffer。随着 context_len 增长，Gather 的数据量线性增长。尤其对于第二个 chunk 起，前面的 K/V 已在上一轮 Gather 过了。

**改进方案（中期）**：
- **增量 Gather + 缓存**：只 Gather 本 chunk 新写入的 K/V，拼接到上一轮的连续 buffer 后面（需要持久化 buffer 跨层共享）。
- **注意**：这需要额外管理持久化 buffer 的生命周期，增加复杂度，建议在性能瓶颈明确后再实施。

### 1.3 Paged Attention Decode Kernel — Shared Memory 限制

**问题**：`paged_attention_kernel_v1` 使用动态 Shared Memory 存储 `smem_logits[max_context_len]`。当 context_len > ~12K 时（48KB / 4B = 12K），会超出默认 Shared Memory 限制。

**改进方案**：
- 实现 **PagedAttention V2**（参考 vLLM）：将 context_len 按 block 分段处理，每段在 Shared Memory 内做局部 softmax，最后 online merge。消除对 context_len 的 shared memory 依赖。

### 1.4 Softmax Warp Reduction — 硬编码 warp 数量

**问题**：`chunked_causal_softmax_kernel` 中 `__shared__ float warp_maxes[8]` 硬编码为 8 个 warp（= 256 threads）。如果未来调整 BLOCK_SIZE 为 512（16 warps），会导致数组越界。

**改进方案**：改为 `warp_maxes[BLOCK_SIZE / 32]` 或使用 cub::BlockReduce（与 decode path 保持一致）。

### 1.5 Engine — 中间 Logits 浪费

**问题**：chunked prefill 的非最后一轮虽然不采样，但仍然计算了完整的 `[chunk_len, vocab_size]` logits 输出。32000 × 512 × 4B ≈ 62MB 的无用计算和显存占用。

**改进方案**：
- 对非最后一轮 chunk，跳过最后的 `lm_head` 线性层（只需要 hidden_state 写入 KV Cache，不需要 logits）。
- 需要在 `ForwardBatch` 中增加 `skip_lm_head` 标志；Model::forward_batched 据此决定是否执行最后的 Linear 投影。

---

## 二、新功能：支持 Qwen 模型

### 2.1 目标

在 NanoInfer 中添加对 Qwen2/Qwen3 模型架构的支持，复用现有的 PagedAttention 和 Continuous Batching 基础设施。

### 2.2 Qwen 与 LLaMA 的架构差异分析

| 差异项 | LLaMA 2 | Qwen2/3 |
|--------|---------|---------|
| Attention Bias | 无 bias | QKV 投影有 bias |
| FFN 结构 | SwiGLU (`gate * silu(up)`) | 同 SwiGLU，但部分版本 hidden_dim 不同 |
| Normalization | RMSNorm | RMSNorm（相同） |
| Position Encoding | RoPE (base=10000) | RoPE (base 可能为 1000000，支持 NTK-aware 扩展) |
| Tokenizer | SentencePiece | tiktoken (BPE) 或 SentencePiece |
| Attention head 配置 | GQA (head_num ≠ kv_head_num) | 同 GQA |
| Vocabulary | 32000 | 151936 (Qwen2) / 更大 |
| 权重命名 | `layers.0.attention.wq.weight` | `model.layers.0.self_attn.q_proj.weight` |

### 2.3 开发步骤

1. **模型导出脚本** (`tools/export_qwen.py`)
   - 从 HuggingFace 权重转换为 NanoInfer 二进制格式
   - 处理 QKV bias 的导出（当前格式只有 weight，需扩展）
   - 处理更大的 vocab_size

2. **二进制格式扩展**
   - 在文件头中增加 `has_qkv_bias` 标志
   - 权重段增加 bias 数据区
   - 兼容现有 LLaMA 格式（向后兼容）

3. **Tokenizer 适配**
   - 新增 tiktoken tokenizer 支持（`TokenizerType::kEncodeTiktoken`）
   - 或继续使用 SentencePiece（Qwen 也提供了 SentencePiece 版本的 tokenizer）

4. **MatMul 层扩展**
   - 支持 `MatMul + Bias` 模式（影响 QKV 和 Output 投影）
   - 可在 MatMul::forward() 中增加可选的 bias tensor

5. **RoPE 扩展**
   - 支持不同的 `rope_theta`（10000 vs 1000000）
   - 支持 NTK-aware Dynamic Scaling（Qwen2 长序列方案）
   - 在 ModelConfig 中增加 `rope_theta_` 字段

6. **模型类**
   - 创建 `include/nanoinfer/model/qwen.h` 和 `src/model/qwen.cpp`
   - 基于 `LLamaModel` 重构公共逻辑到 `Model` 基类
   - Qwen 子类覆写差异部分（bias MatMul、不同 RoPE config）

7. **测试**
   - 小规模 Qwen2-0.5B 或 Qwen2-1.5B 验证
   - 对比 HuggingFace transformers 参考输出

### 2.4 预计工作量

约 3-5 天，核心改动在导出脚本和 MatMul bias 支持，架构复用现有代码。

---

## 三、新功能：调度策略改进 (kPriority)

### 3.1 当前现状

Scheduler 仅实现 `kFCFS`（先来先服务），所有请求按到达顺序处理。这在生产环境中不够灵活。

### 3.2 优先级调度设计思路

#### 3.2.1 优先级来源

```cpp
struct RequestPriority {
    int32_t level = 0;          // 用户指定优先级 (0=最高)
    double deadline_seconds;     // SLO deadline (可选)
    bool is_vip = false;        // VIP 请求标记
};
```

优先级因素（可组合加权）：
- **用户指定优先级**：API 层调用 `add_request(prompt, max_tokens, priority)` 时传入
- **剩余 Prompt 长度**：短 prompt 优先（Shortest Job First），减少平均延迟
- **等待时间**：防止低优先级请求饥饿（Aging 机制）
- **SLO Deadline**：离 deadline 最近的优先（Earliest Deadline First）

#### 3.2.2 调度算法

```
schedule_next_batch():
  Phase 1: 保留所有 RUNNING 请求（不可中断）
  Phase 2: 对 WAITING 队列按优先级排序
  Phase 3: 贪心填充至 max_batch_size
    - 如果启用抢占(Preemption): 考虑踢出低优先级 RUNNING 请求
    - 如果不启用: 仅从 WAITING 队列取
```

#### 3.2.3 抢占机制 (Preemption)

高优先级请求到达但 batch 已满时的处理：
- **Swap**：将低优先级请求的 KV Cache 从 GPU 换出到 CPU，释放 GPU blocks
- **Recompute**：直接丢弃低优先级请求的 KV Cache，下次恢复时 recompute prefill
- **需要的基础设施**：
  - `InferenceRequest` 增加 `preempt()` / `resume()` 状态转换（已预留）
  - `KVCacheManager` 增加 `swap_out(seq_id)` / `swap_in(seq_id)` 接口
  - CPU-side KV Cache 存储池

#### 3.2.4 实现步骤

1. `InferenceRequest` 增加 `RequestPriority` 字段
2. `Scheduler` 增加优先级队列（`std::priority_queue` 或排序的 deque）
3. `schedule_next_batch()` 中 Phase 2 改为按优先级排序选取
4. 增加 Aging 参数（每等一轮 step，优先级提升 delta）
5. （可选）实现 Preemption + KV Cache Swap

### 3.3 预计工作量

基础优先级调度：1-2 天。抢占机制（含 KV Swap）：额外 3-5 天。

---

## 四、新功能：多样化采样策略

### 4.1 当前现状

仅实现 `ArgmaxSampler`（贪心采样），导致：
- 输出确定性（多次运行相同输入，输出完全一致）
- 容易陷入退化重复循环（如 "The meaning of life is to be happy. 2. The meaning..." 的问题）
- 无法控制生成多样性

### 4.2 计划实现的采样策略

#### 4.2.1 Temperature Sampling

```
P(token_i) = softmax(logit_i / temperature)
```
- temperature > 1.0：更均匀（创意写作）
- temperature < 1.0：更尖锐（事实回答）
- temperature → 0：退化为 Argmax

**实现**：一个简单的 CUDA kernel，对 logits 逐元素除以 temperature，然后做 softmax + 多项式采样。

#### 4.2.2 Top-K Sampling

限制候选词表为概率最高的 K 个 token，其余置零后重新归一化。

**实现**：
- CUDA Top-K 算法（基于 radix sort 或 partial sort）
- K 值可配置（常见 k=50）

#### 4.2.3 Top-P (Nucleus) Sampling

按概率从高到低累加，保留累积概率 ≤ p 的最小 token 集合。

**实现**：
- 先排序 logits → 计算 cumsum → 截断 → 重新归一化 → 多项式采样
- 常见 p=0.9

#### 4.2.4 Repetition Penalty

对已生成过的 token 施加惩罚：
```
logit'_i = logit_i / penalty  (if token_i in generated_tokens)
```

**实现**：
- 维护 `generated_tokens` 集合（per-request）
- 采样前对 logits 施加 penalty kernel

#### 4.2.5 统一采样接口

```cpp
struct SamplingParams {
    float temperature = 1.0f;
    int32_t top_k = -1;           // -1 表示不使用
    float top_p = 1.0f;           // 1.0 表示不使用
    float repetition_penalty = 1.0f;
    int64_t seed = -1;            // 随机种子，-1 表示随机
};

class ConfigurableSampler : public Sampler {
    void sample_batched(const tensor::Tensor& logits,
                        tensor::Tensor& output_ids,
                        const std::vector<SamplingParams>& params,  // per-request
                        void* stream = nullptr);
};
```

### 4.3 实现步骤

1. 定义 `SamplingParams` 结构体
2. 实现 Temperature Scaling kernel（最简单）
3. 实现 Top-K kernel（基于 cub::DeviceRadixSort 或 bitonic sort）
4. 实现 Top-P kernel（排序 + cumsum + 截断）
5. 实现 Repetition Penalty kernel
6. 组合为 `ConfigurableSampler`：temperature → top_k → top_p → rep_penalty → multinomial
7. 添加 cuRAND 随机数生成器用于多项式采样
8. Engine 层适配：`add_request()` 接受 `SamplingParams`，采样时传入

### 4.4 预计工作量

Temperature + Top-K + Top-P 基础版本：2-3 天。Repetition Penalty + 完整接口：额外 1-2 天。

---

## 五、深度性能优化

### 5.1 FP16 / BF16 推理

**现状**：全 FP32 推理，模型 4.4GB + KV Cache 704 MB。

**目标**：FP16 推理可将显存减半、吞吐翻倍。

**步骤**：
1. 模型导出脚本支持 FP16 权重导出
2. 所有 CUDA kernel 添加 `__half` / `half2` 模板特化
3. cuBLAS 调用切换为 `cublasHgemm` / `cublasSgemmEx`（TensorCore 加速）
4. KV Cache 改为 FP16 存储（显存减半）
5. 混合精度：Embedding + LM Head 保持 FP32，中间层 FP16

**预计工作量**：3-5 天（kernel 模板化 + 导出工具修改）

### 5.2 KV Cache 量化 (INT8/FP8)

**目标**：KV Cache 从 FP16 进一步压缩到 INT8，显存再减半。

**方案**：
- Per-token 量化：每个 token 的 KV 向量独立量化
- `cache_value = (fp_value - zero_point) / scale`
- 写入 cache 时量化，读出时反量化
- 参考 vLLM 的 KV Cache INT8 方案

### 5.3 FlashAttention 集成

**目标**：替换当前 cuBLAS GEMM + 显式 softmax 的两步方案，一步完成 fused attention。

**方案选项**：
1. **FlashAttention-2 库集成**：直接链接 FlashAttention 库
2. **Flash-Decoding**：对 decode 阶段的 PagedAttention，将长 context 分段并行处理
3. **Triton / CUTLASS**：使用 CUTLASS 3.x 的 fused attention template

**收益**：
- 消除 scores 矩阵的显存分配（完全 fused）
- 更好的 warp-level 并行度
- 预计 2-3x 吞吐提升

### 5.4 CUDA Graph

**目标**：消除 decode 阶段每步的 CPU kernel launch overhead。

**原理**：decode 阶段每步的计算图完全一致（固定 batch_size, 固定 seq_len=1），可以用 CUDA Graph 捕获一次，后续 replay。

**步骤**：
1. 在首次 decode step 时启用 `cudaStreamBeginCapture`
2. 执行完整 forward pass
3. `cudaStreamEndCapture` 获得 `cudaGraph_t`
4. 后续 decode step 直接 `cudaGraphLaunch`
5. 当 batch 组成变化时（请求加入/退出），重新捕获

**挑战**：动态 batch size 变化时需要重新捕获 graph。

### 5.5 Continuous Batching 优化 — Prefill/Decode 混合

**目标**：当前 prefill 和 decode 在同一 step 内串行执行。可以用 multi-stream 并行。

**方案**：
- Prefill 请求在 stream_1 上执行
- Decode batch 在 stream_2 上执行
- 两者共享 KV Cache（不同序列的 blocks 无冲突，天然安全）
- 仅在采样阶段同步

### 5.6 Speculative Decoding

**目标**：用小模型（Draft Model）一次猜测多个 token，大模型（Target Model）并行验证，提高 decode 吞吐。

**预计收益**：2-3x decode 速度提升（取决于 draft model 的接受率）

**步骤**：
1. 支持同时加载两个模型（Draft + Target）
2. Draft model 自回归生成 K 个候选 token
3. Target model 一次 forward 验证所有 K 个 token
4. 基于概率分布决定接受前 N 个 token（N ≤ K）
5. 集成到 Engine 的 decode 循环

---

## 六、工程化改进

### 6.1 错误处理与 CUDA 错误检查

**问题**：当前许多 CUDA API 调用（cudaMallocAsync, cudaMemcpyAsync 等）没有检查返回值。

**改进**：
```cpp
#define CUDA_CHECK(call)                                       \
    do {                                                       \
        cudaError_t err = (call);                              \
        CHECK(err == cudaSuccess) << cudaGetErrorString(err);  \
    } while (0)
```

### 6.2 Profiling 基础设施

- 集成 NVTX 标注，方便 Nsight Systems 分析
- 每层/每 kernel 的耗时统计
- 显存使用追踪

### 6.3 单元测试完善

- 为 chunked prefill kernel 添加 correctness test（对比单步 prefill 结果）
- 为 Engine chunked prefill 添加端到端测试（长 prompt 分块 vs 短 prompt 一次性）
- 边界条件测试（chunk_size = 1, chunk_size > prompt_len）

### 6.4 配置系统

- 统一的运行时配置文件（JSON/YAML）
- 命令行参数解析（替代硬编码常量）
- 支持动态调整 chunk_size, max_batch_size 等

---

## 七、优先级与时间线建议

| 优先级 | 任务 | 预计工作量 | 依赖 |
|--------|------|-----------|------|
| P0 (高) | 预分配 workspace 替代 cudaMallocAsync | 1 天 | 无 |
| P0 (高) | CUDA 错误检查宏 | 0.5 天 | 无 |
| P1 (中) | 采样策略 (Temperature + Top-P) | 2-3 天 | 无 |
| P1 (中) | 非最后 chunk 跳过 LM Head | 1 天 | 无 |
| P1 (中) | FP16 推理 | 3-5 天 | 无 |
| P2 | Qwen 模型支持 | 3-5 天 | FP16 (可选) |
| P2 | Priority 调度 | 1-2 天 | 无 |
| P2 | Chunked Prefill 单元测试 | 1 天 | 无 |
| P3 (低) | FlashAttention 集成 | 5-7 天 | FP16 |
| P3 (低) | CUDA Graph | 3-5 天 | 无 |
| P3 (低) | Speculative Decoding | 5-7 天 | 多模型加载 |
| P3 (低) | Priority + Preemption (KV Swap) | 3-5 天 | Priority 调度 |
