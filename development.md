# NanoInfer 开发计划

> Generated By Cladue Opus 4.6

本文档记录 NanoInfer 下一阶段的开发方向，包括代码审查发现的改进点、新功能规划和性能优化路线。

---

## 一、代码审查：当前发现的问题和改进点

### 1.1 Chunked Prefill Kernel — 每层 7 次 cudaMallocAsync

**问题**：`prefill_attention_kernel` 中每次调用（即每层、每 chunk）都执行 7 次 `cudaMallocAsync` + `cudaFreeAsync`（k_gathered, v_gathered, q_reshaped, k_reshaped, v_reshaped, scores, out_reshaped）。对 22 层模型，单个 chunk 就是 154 次 async alloc/free。

**改进方案**：
1. **预分配工作空间**：在 Engine 或 Model 初始化时，根据 `max(chunk_size × context_budget)` 预分配一块固定大小的 workspace buffer，kernel 内部用偏移量取子区间，避免运行时动态分配。
2. **双缓冲（Ping-Pong）**：如果需要支持多个 prefill 请求并行（CUDA multi-stream），可分配两组 workspace 交替使用。
3. **预计算最大尺寸**：`max_scores_size = num_heads × chunk_size × max_seq_len × sizeof(float)`，初始化时一次性 malloc。

### 1.2 Chunked Prefill — Gather 全量 K/V 效率

**问题**：每层 prefill 时都从 Paged Cache 中 Gather `[0, context_len)` 的全部 K/V 到连续 buffer。随着 context_len 增长，Gather 的数据量线性增长。尤其对于第二个 chunk 起，前面的 K/V 已在上一轮 Gather 过了。

**改进方案（中期）**：
- **增量 Gather + 缓存**：只 Gather 本 chunk 新写入的 K/V，拼接到上一轮的连续 buffer 后面（需要持久化 buffer 跨层共享）。
- **注意**：这需要额外管理持久化 buffer 的生命周期，增加复杂度，建议在性能瓶颈明确后再实施。

### 1.3 Paged Attention Decode Kernel — Shared Memory 限制

**问题**：`paged_attention_kernel_v1` 使用动态 Shared Memory 存储 `smem_logits[max_context_len]`。当 context_len > ~12K 时（48KB / 4B = 12K），会超出默认 Shared Memory 限制。

**改进方案**：
- 实现 **PagedAttention V2**（参考 vLLM）：将 context_len 按 block 分段处理，每段在 Shared Memory 内做局部 softmax，最后 online merge。消除对 context_len 的 shared memory 依赖。

### 1.4 Softmax Warp Reduction — 硬编码 warp 数量

**问题**：`chunked_causal_softmax_kernel` 中 `__shared__ float warp_maxes[8]` 硬编码为 8 个 warp（= 256 threads）。如果未来调整 BLOCK_SIZE 为 512（16 warps），会导致数组越界。

**改进方案**：改为 `warp_maxes[BLOCK_SIZE / 32]` 或使用 cub::BlockReduce（与 decode path 保持一致）。

### 1.5 Engine — 中间 Logits 浪费

**问题**：chunked prefill 的非最后一轮虽然不采样，但仍然计算了完整的 `[chunk_len, vocab_size]` logits 输出。32000 × 512 × 4B ≈ 62MB 的无用计算和显存占用。

**改进方案**：
- 对非最后一轮 chunk，跳过最后的 `lm_head` 线性层（只需要 hidden_state 写入 KV Cache，不需要 logits）。
- 需要在 `ForwardBatch` 中增加 `skip_lm_head` 标志；Model::forward_batched 据此决定是否执行最后的 Linear 投影。

---

## 二、新功能：支持 Qwen 模型

### 2.1 目标

在 NanoInfer 中添加对 Qwen2/Qwen3 模型架构的支持，复用现有的 PagedAttention 和 Continuous Batching 基础设施。

### 2.2 Qwen 与 LLaMA 的架构差异分析

| 差异项 | LLaMA 2 | Qwen2/3 |
|--------|---------|---------|
| Attention Bias | 无 bias | QKV 投影有 bias |
| FFN 结构 | SwiGLU (`gate * silu(up)`) | 同 SwiGLU，但部分版本 hidden_dim 不同 |
| Normalization | RMSNorm | RMSNorm（相同） |
| Position Encoding | RoPE (base=10000) | RoPE (base 可能为 1000000，支持 NTK-aware 扩展) |
| Tokenizer | SentencePiece | tiktoken (BPE) 或 SentencePiece |
| Attention head 配置 | GQA (head_num ≠ kv_head_num) | 同 GQA |
| Vocabulary | 32000 | 151936 (Qwen2) / 更大 |
| 权重命名 | `layers.0.attention.wq.weight` | `model.layers.0.self_attn.q_proj.weight` |

### 2.3 开发步骤

1. **模型导出脚本** (`tools/export_qwen.py`)
   - 从 HuggingFace 权重转换为 NanoInfer 二进制格式
   - 处理 QKV bias 的导出（当前格式只有 weight，需扩展）
   - 处理更大的 vocab_size

2. **二进制格式扩展**
   - 在文件头中增加 `has_qkv_bias` 标志
   - 权重段增加 bias 数据区
   - 兼容现有 LLaMA 格式（向后兼容）

3. **Tokenizer 适配**
   - 新增 tiktoken tokenizer 支持（`TokenizerType::kEncodeTiktoken`）
   - 或继续使用 SentencePiece（Qwen 也提供了 SentencePiece 版本的 tokenizer）

4. **MatMul 层扩展**
   - 支持 `MatMul + Bias` 模式（影响 QKV 和 Output 投影）
   - 可在 MatMul::forward() 中增加可选的 bias tensor

5. **RoPE 扩展**
   - 支持不同的 `rope_theta`（10000 vs 1000000）
   - 支持 NTK-aware Dynamic Scaling（Qwen2 长序列方案）
   - 在 ModelConfig 中增加 `rope_theta_` 字段

6. **模型类**
   - 创建 `include/nanoinfer/model/qwen.h` 和 `src/model/qwen.cpp`
   - 基于 `LLamaModel` 重构公共逻辑到 `Model` 基类
   - Qwen 子类覆写差异部分（bias MatMul、不同 RoPE config）

7. **测试**
   - 小规模 Qwen2-0.5B 或 Qwen2-1.5B 验证
   - 对比 HuggingFace transformers 参考输出

### 2.4 预计工作量

约 3-5 天，核心改动在导出脚本和 MatMul bias 支持，架构复用现有代码。

---

## 三、新功能：调度策略改进 (kPriority)

### 3.1 当前现状

Scheduler 仅实现 `kFCFS`（先来先服务），所有请求按到达顺序处理。这在生产环境中不够灵活。

### 3.2 优先级调度设计思路

#### 3.2.1 优先级来源

```cpp
struct RequestPriority {
    int32_t level = 0;          // 用户指定优先级 (0=最高)
    double deadline_seconds;     // SLO deadline (可选)
    bool is_vip = false;        // VIP 请求标记
};
```

优先级因素（可组合加权）：
- **用户指定优先级**：API 层调用 `add_request(prompt, max_tokens, priority)` 时传入
- **剩余 Prompt 长度**：短 prompt 优先（Shortest Job First），减少平均延迟
- **等待时间**：防止低优先级请求饥饿（Aging 机制）
- **SLO Deadline**：离 deadline 最近的优先（Earliest Deadline First）

#### 3.2.2 调度算法

```
schedule_next_batch():
  Phase 1: 保留所有 RUNNING 请求（不可中断）
  Phase 2: 对 WAITING 队列按优先级排序
  Phase 3: 贪心填充至 max_batch_size
    - 如果启用抢占(Preemption): 考虑踢出低优先级 RUNNING 请求
    - 如果不启用: 仅从 WAITING 队列取
```

#### 3.2.3 抢占机制 (Preemption)

高优先级请求到达但 batch 已满时的处理：
- **Swap**：将低优先级请求的 KV Cache 从 GPU 换出到 CPU，释放 GPU blocks
- **Recompute**：直接丢弃低优先级请求的 KV Cache，下次恢复时 recompute prefill
- **需要的基础设施**：
  - `InferenceRequest` 增加 `preempt()` / `resume()` 状态转换（已预留）
  - `KVCacheManager` 增加 `swap_out(seq_id)` / `swap_in(seq_id)` 接口
  - CPU-side KV Cache 存储池

#### 3.2.4 实现步骤

1. `InferenceRequest` 增加 `RequestPriority` 字段
2. `Scheduler` 增加优先级队列（`std::priority_queue` 或排序的 deque）
3. `schedule_next_batch()` 中 Phase 2 改为按优先级排序选取
4. 增加 Aging 参数（每等一轮 step，优先级提升 delta）
5. （可选）实现 Preemption + KV Cache Swap

### 3.3 预计工作量

基础优先级调度：1-2 天。抢占机制（含 KV Swap）：额外 3-5 天。

---

## 四、新功能：多样化采样策略

### 4.1 当前现状

仅实现 `ArgmaxSampler`（贪心采样），导致：
- 输出确定性（多次运行相同输入，输出完全一致）
- 容易陷入退化重复循环（如 "The meaning of life is to be happy. 2. The meaning..." 的问题）
- 无法控制生成多样性

### 4.2 计划实现的采样策略

#### 4.2.1 Temperature Sampling

```
P(token_i) = softmax(logit_i / temperature)
```
- temperature > 1.0：更均匀（创意写作）
- temperature < 1.0：更尖锐（事实回答）
- temperature → 0：退化为 Argmax

**实现**：一个简单的 CUDA kernel，对 logits 逐元素除以 temperature，然后做 softmax + 多项式采样。

#### 4.2.2 Top-K Sampling

限制候选词表为概率最高的 K 个 token，其余置零后重新归一化。

**实现**：
- CUDA Top-K 算法（基于 radix sort 或 partial sort）
- K 值可配置（常见 k=50）

#### 4.2.3 Top-P (Nucleus) Sampling

按概率从高到低累加，保留累积概率 ≤ p 的最小 token 集合。

**实现**：
- 先排序 logits → 计算 cumsum → 截断 → 重新归一化 → 多项式采样
- 常见 p=0.9

#### 4.2.4 Repetition Penalty

对已生成过的 token 施加惩罚：
```
logit'_i = logit_i / penalty  (if token_i in generated_tokens)
```

**实现**：
- 维护 `generated_tokens` 集合（per-request）
- 采样前对 logits 施加 penalty kernel

#### 4.2.5 统一采样接口

```cpp
struct SamplingParams {
    float temperature = 1.0f;
    int32_t top_k = -1;           // -1 表示不使用
    float top_p = 1.0f;           // 1.0 表示不使用
    float repetition_penalty = 1.0f;
    int64_t seed = -1;            // 随机种子，-1 表示随机
};

class ConfigurableSampler : public Sampler {
    void sample_batched(const tensor::Tensor& logits,
                        tensor::Tensor& output_ids,
                        const std::vector<SamplingParams>& params,  // per-request
                        void* stream = nullptr);
};
```

### 4.3 实现步骤

1. 定义 `SamplingParams` 结构体
2. 实现 Temperature Scaling kernel（最简单）
3. 实现 Top-K kernel（基于 cub::DeviceRadixSort 或 bitonic sort）
4. 实现 Top-P kernel（排序 + cumsum + 截断）
5. 实现 Repetition Penalty kernel
6. 组合为 `ConfigurableSampler`：temperature → top_k → top_p → rep_penalty → multinomial
7. 添加 cuRAND 随机数生成器用于多项式采样
8. Engine 层适配：`add_request()` 接受 `SamplingParams`，采样时传入

### 4.4 预计工作量

Temperature + Top-K + Top-P 基础版本：2-3 天。Repetition Penalty + 完整接口：额外 1-2 天。

---

## 五、深度性能优化

### 5.1 FP16 / BF16 推理

**现状**：全 FP32 推理，模型 4.4GB + KV Cache 704 MB。

**目标**：FP16 推理可将显存减半、吞吐翻倍。

**步骤**：
1. 模型导出脚本支持 FP16 权重导出
2. 所有 CUDA kernel 添加 `__half` / `half2` 模板特化
3. cuBLAS 调用切换为 `cublasHgemm` / `cublasSgemmEx`（TensorCore 加速）
4. KV Cache 改为 FP16 存储（显存减半）
5. 混合精度：Embedding + LM Head 保持 FP32，中间层 FP16

**预计工作量**：3-5 天（kernel 模板化 + 导出工具修改）

### 5.2 KV Cache 量化 (INT8/FP8)

**目标**：KV Cache 从 FP16 进一步压缩到 INT8，显存再减半。

**方案**：
- Per-token 量化：每个 token 的 KV 向量独立量化
- `cache_value = (fp_value - zero_point) / scale`
- 写入 cache 时量化，读出时反量化
- 参考 vLLM 的 KV Cache INT8 方案

### 5.3 FlashAttention 集成

**目标**：替换当前 cuBLAS GEMM + 显式 softmax 的两步方案，一步完成 fused attention。

**方案选项**：
1. **FlashAttention-2 库集成**：直接链接 FlashAttention 库
2. **Flash-Decoding**：对 decode 阶段的 PagedAttention，将长 context 分段并行处理
3. **Triton / CUTLASS**：使用 CUTLASS 3.x 的 fused attention template

**收益**：
- 消除 scores 矩阵的显存分配（完全 fused）
- 更好的 warp-level 并行度
- 预计 2-3x 吞吐提升

### 5.4 CUDA Graph

**目标**：消除 decode 阶段每步的 CPU kernel launch overhead。

**原理**：decode 阶段每步的计算图完全一致（固定 batch_size, 固定 seq_len=1），可以用 CUDA Graph 捕获一次，后续 replay。

**步骤**：
1. 在首次 decode step 时启用 `cudaStreamBeginCapture`
2. 执行完整 forward pass
3. `cudaStreamEndCapture` 获得 `cudaGraph_t`
4. 后续 decode step 直接 `cudaGraphLaunch`
5. 当 batch 组成变化时（请求加入/退出），重新捕获

**挑战**：动态 batch size 变化时需要重新捕获 graph。

### 5.5 Continuous Batching 优化 — Prefill/Decode 混合

**目标**：当前 prefill 和 decode 在同一 step 内串行执行。可以用 multi-stream 并行。

**方案**：
- Prefill 请求在 stream_1 上执行
- Decode batch 在 stream_2 上执行
- 两者共享 KV Cache（不同序列的 blocks 无冲突，天然安全）
- 仅在采样阶段同步

### 5.6 Speculative Decoding

**目标**：用小模型（Draft Model）一次猜测多个 token，大模型（Target Model）并行验证，提高 decode 吞吐。

**预计收益**：2-3x decode 速度提升（取决于 draft model 的接受率）

**步骤**：
1. 支持同时加载两个模型（Draft + Target）
2. Draft model 自回归生成 K 个候选 token
3. Target model 一次 forward 验证所有 K 个 token
4. 基于概率分布决定接受前 N 个 token（N ≤ K）
5. 集成到 Engine 的 decode 循环

---

## 六、CPU 推理优化

### 6.1 当前基线

CPU 推理已功能完整（Chunked Prefill + PagedAttention Decode + Continuous Batching），在 FP32 下对 TinyLLaMA-1.1B (22 layers, dim=2048) 的实测性能：

- **Decode 吞吐**：~1.09 tok/s（2 并发请求）
- **端到端吞吐**：~1.43 tok/s（含 Prefill）
- **瓶颈分析**：MatMul 占 ~90%+ 时间（每层 4 次 [1×2048] × [2048×N] = 7 次/layer 含 QKV+O+FFN）

### 6.2 矩阵乘法优化（最高优先级）

当前 CPU MatMul 是朴素的三重循环（O(N³) 无优化）。这是性能瓶颈的根源。

#### 6.2.1 OpenBLAS / MKL 集成

**方案**：将 `matmul_kernel_cpu` 替换为 `cblas_sgemm` 调用。

```cpp
// 当前: 三重循环
// 优化: cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasTrans,
//                   M, N, K, 1.0f, A, K, B, K, 0.0f, C, N);
```

**预计收益**：10-50x 加速（OpenBLAS 利用 AVX-512/AVX2 + 分块 + 多线程）

**实现步骤**：
1. CMakeLists.txt 通过 `find_package(BLAS)` 查找系统 OpenBLAS
2. `matmul_kernel_cpu` 中调用 `cblas_sgemm`
3. 确保与现有权重布局（Row-Major / Column-Major）的 transpose 标记匹配

#### 6.2.2 手写 SIMD Kernel（备选）

如果不想依赖外部库：
- AVX2: `_mm256_fmadd_ps` 做 8-wide FMA
- 4×8 micro-kernel + cache-line 友好的分块（64×64 或 128×64）
- 预计比朴素循环快 8-20x

### 6.3 多线程并行

#### 6.3.1 层内并行 (Intra-Layer)

**Attention Heads 并行**：`prefill_attention_kernel_cpu` 和 `paged_attention_kernel_cpu` 中的 head 循环可以 `#pragma omp parallel for` 并行化（各 head 完全独立）。

```cpp
#pragma omp parallel for schedule(static)
for (int32_t head = 0; head < num_heads; ++head) {
    // 每个 head 的 Q@K^T → softmax → V 完全独立
}
```

**MatMul 多线程**：如果使用 OpenBLAS，自带多线程；如果手写可用 OpenMP 按行分块并行。

#### 6.3.2 Batch 并行

Decode 阶段多个序列可以在不同线程上并行处理（各序列写不同的 KV Cache block，无冲突）。

### 6.4 内存优化

#### 6.4.1 权重量化 (INT8/INT4)

**目标**：将 FP32 权重离线量化为 INT8，推理时用 INT8 MatMul + FP32 Accumulate。

**收益**：
- 模型体积 4.4GB → 1.1GB（INT8）或 550MB（INT4）
- 内存带宽减少 4x，CPU 推理的 memory-bound 瓶颈显著缓解
- 利用 AVX-VNNI（INT8 点积加速指令）

**实现**：
1. 导出工具增加 INT8 权重导出（Per-Channel 量化）
2. 新增 `matmul_int8_kernel_cpu`：INT8 × INT8 → INT32 → FP32（反量化）
3. 可用 `oneDNN` 库的 INT8 MatMul kernel

#### 6.4.2 Memory Mapping (mmap)

**目标**：使用 `mmap` 加载模型权重，避免 `read()` 的额外内存拷贝。

**收益**：
- 启动时间大幅缩短（不需要一次性读入全部权重到 malloc 内存）
- OS 负责 page-in/page-out，多进程可共享同一份物理内存
- 对小内存机器友好（lazy loading）

**实现**：
1. `raw_model_data.cpp` 中用 `mmap(fd, PROT_READ, MAP_PRIVATE)` 替换 `fread`
2. 权重 Tensor 直接指向 mmap 区域，`allocate = false`

### 6.5 Prefill 优化

#### 6.5.1 分块 MatMul Prefill

当前 Prefill 每个 query token 独立与全部 K 做 dot product。可以利用 BLAS 做批量 GEMM：

```cpp
// Query: [chunk_len, num_heads * head_size]
// K_gathered: [context_len, num_kv_heads * head_size]
// Scores = Q @ K^T → [chunk_len, context_len] (per head group)
cblas_sgemm(..., chunk_len, context_len, head_size, ...);
```

这比逐 token 循环快得多（利用了 BLAS 的微架构优化）。

#### 6.5.2 Causal Mask 优化

当前 Prefill CPU kernel 对每个 query token 的 valid_len 不同（causal），导致循环边界不规则。可以用 Masked GEMM（全量计算 + mask 后 softmax）减少分支。

### 6.6 算子融合 (Operator Fusion)

#### 6.6.1 RMSNorm + Linear 融合

将 RMSNorm 归一化后直接进入下一步的 MatMul，避免中间 tensor 的内存写回 + 读取。

#### 6.6.2 SwiGLU 融合

当前 SwiGLU 分 3 步（gate_proj, up_proj, silu + mul）。可融合为一个 kernel，减少内存访问。

### 6.7 CPU 推理优先级与时间线

| 优先级 | 任务 | 预计收益 | 预计工作量 |
|--------|------|---------|-----------|
| **P0 (最高)** | 集成 OpenBLAS for MatMul | 10-50x MatMul 加速 | 0.5-1 天 |
| **P0** | Attention heads OpenMP 并行 | 线性加速（按核心数） | 0.5 天 |
| **P1** | Prefill 用 cblas_sgemm 做批量 attention | 5-10x prefill 加速 | 1 天 |
| **P1** | 权重 INT8 量化 | 4x 内存带宽节省 | 2-3 天 |
| **P2** | Memory Mapping (mmap) 加载权重 | 启动速度提升 | 0.5 天 |
| **P2** | SwiGLU / RMSNorm 算子融合 | 10-20% 整体加速 | 1-2 天 |
| **P3** | 手写 AVX2 micro-kernel | 无外部依赖的高性能 MatMul | 3-5 天 |

**最优投入顺序**：先集成 OpenBLAS（投入产出比最高），再加 OpenMP 多线程，预计可实现 **20-100x** 整体推理加速（从 1 tok/s → 20-100 tok/s）。

---

## 七、工程化改进

### 6.1 错误处理与 CUDA 错误检查

**问题**：当前许多 CUDA API 调用（cudaMallocAsync, cudaMemcpyAsync 等）没有检查返回值。

**改进**：
```cpp
#define CUDA_CHECK(call)                                       \
    do {                                                       \
        cudaError_t err = (call);                              \
        CHECK(err == cudaSuccess) << cudaGetErrorString(err);  \
    } while (0)
```

### 6.2 Profiling 基础设施

- 集成 NVTX 标注，方便 Nsight Systems 分析
- 每层/每 kernel 的耗时统计
- 显存使用追踪

### 6.3 单元测试完善

- 为 chunked prefill kernel 添加 correctness test（对比单步 prefill 结果）
- 为 Engine chunked prefill 添加端到端测试（长 prompt 分块 vs 短 prompt 一次性）
- 边界条件测试（chunk_size = 1, chunk_size > prompt_len）

### 6.4 配置系统

- 统一的运行时配置文件（JSON/YAML）
- 命令行参数解析（替代硬编码常量）
- 支持动态调整 chunk_size, max_batch_size 等

---

## 八、多卡并行推理 (Multi-GPU Tensor Parallelism)

### 8.1 背景与目标

**问题**：大参数量模型（如 LLaMA-3 70B 全精度约 280 GB、FP16 约 140 GB）无法装入单张 GPU（A100 80GB）。  
**目标**：通过 **张量并行 (Tensor Parallelism, TP)** 将模型权重水平切分到多卡上，每张 GPU 只保存 1/N 的权重，实现超出单卡显存上限的模型推理。

本方案聚焦 **TP** 策略（行/列并行线性层），适合 NanoInfer 现有的单机多卡（NVLink 或 PCIe）场景，无需修改调度器与 KV Cache 管理逻辑。

---

### 8.2 并行策略选型

| 策略 | 适用场景 | 通信量 | 实现复杂度 |
|---|---|---|---|
| **Tensor Parallelism (TP)** | 单机多卡，权重切分 | O(B×S×D/step) AllReduce | 中 |
| Pipeline Parallelism (PP) | 超多层模型，逐层切分 | 点对点激活传输 | 高（需 micro-batch 流水线） |
| Sequence Parallelism | 超长序列 (>32K) | 同 TP | 高（KV Cache 须分段） |

**结论**：首期只实现 TP，PP 作为长期规划预留接口。

---

### 8.3 Tensor Parallelism 数学原理

#### 8.3.1 列并行 Linear（Column-Parallel）

将权重矩阵 $W \in \mathbb{R}^{K \times N}$ 沿 **输出维度** 按 GPU 数切分：

$$W = [W_0 \mid W_1 \mid \cdots \mid W_{P-1}], \quad W_i \in \mathbb{R}^{K \times (N/P)}$$

每卡本地计算 $Y_i = X W_i$，输出 $Y_i \in \mathbb{R}^{B \times (N/P)}$，**无需通信**（结果在输出维度上拼接）。

适用于：Q/K/V 投影、FFN `gate_proj`、FFN `up_proj`。

#### 8.3.2 行并行 Linear（Row-Parallel）

将权重矩阵 $W \in \mathbb{R}^{K \times N}$ 沿 **输入维度** 切分（即每卡持有不同的输入分片）：

$$W = \begin{bmatrix} W_0 \\ W_1 \\ \vdots \\ W_{P-1} \end{bmatrix}, \quad W_i \in \mathbb{R}^{(K/P) \times N}$$

每卡计算 $Y_i = X_i W_i$（$X_i$ 是上一层列并行的本卡输出），最终需要 **AllReduce** 聚合：$Y = \sum_i Y_i$。

适用于：Attention Output 投影、FFN `down_proj`。

#### 8.3.3 每个 Transformer 层的通信模式

```
Input X (replicated on all GPUs)
  │
  ├─ [Column-Parallel] QKV Proj  → Y_qkv_i (local shard)
  │       no comm needed
  │
  ├─ Attention (each GPU handles num_heads/P heads)
  │
  ├─ [Row-Parallel]   O Proj     → Y_o_i (partial sum)
  │       ──AllReduce──→  Y_o (replicated)
  │
  ├─ RMSNorm (local, replicated)
  │
  ├─ [Column-Parallel] gate/up   → Y_gate_i, Y_up_i (local)
  │       SwiGLU (local)
  │
  └─ [Row-Parallel]   down_proj  → Y_ffn_i (partial sum)
          ──AllReduce──→  Y_ffn (replicated)
```

每层只需 **2 次 AllReduce**（O_proj 后 + down_proj 后），通信量为 $2 \times B \times S \times D \times \text{sizeof(float)}$，与层数无关。

---

### 8.4 代码改动方案

#### 8.4.1 新增基础设施：NCCL 通信器封装

新建 `include/nanoinfer/base/comm.h` 和 `src/base/comm.cpp`：

```cpp
namespace base {

/// 封装单个进程（GPU）的通信上下文
struct CommContext {
    int rank;          ///< 当前 GPU rank
    int world_size;    ///< 总 GPU 数
    ncclComm_t comm;   ///< NCCL communicator
    cudaStream_t stream; ///< 专用通信 stream，与计算 stream 分离
};

/// 全局单例（进程内唯一）
class CommManager {
public:
    static CommManager& instance();
    /// 初始化：从环境变量 RANK / WORLD_SIZE / MASTER_ADDR 读取配置
    base::Status init(int rank, int world_size, const std::string& master_addr, int master_port);
    void all_reduce_sum(void* buf, size_t count, ncclDataType_t dtype);
    void broadcast(void* buf, size_t count, ncclDataType_t dtype, int root);
    const CommContext& ctx() const { return ctx_; }
private:
    CommContext ctx_;
};

} // namespace base
```

**依赖**：CMakeLists.txt 通过 `find_package(NCCL REQUIRED)` 链接 `libnccl.so`。

---

#### 8.4.2 新增算子：并行 Linear 层

新建 `include/nanoinfer/op/parallel_linear.h`：

```cpp
namespace op {

/// 列并行 Linear：每卡持有 W[:, local_start:local_end]
/// 输出为本卡的局部分片，无 AllReduce
class ColumnParallelLinear : public Layer {
public:
    explicit ColumnParallelLinear(base::DeviceType device,
                                   int32_t in_features, int32_t out_features,
                                   bool has_bias = false);
    base::Status forward(const tensor::Tensor& input, tensor::Tensor& output) override;
    // 权重形状: [in_features, out_features/world_size]（本卡分片）
};

/// 行并行 Linear：每卡持有 W[local_start:local_end, :]
/// 前向结束后执行 AllReduce，输出在所有卡上 replicate
class RowParallelLinear : public Layer {
public:
    explicit RowParallelLinear(base::DeviceType device,
                                int32_t in_features, int32_t out_features,
                                bool has_bias = false);
    base::Status forward(const tensor::Tensor& input, tensor::Tensor& output) override;
    // 内部在 output 上调用 CommManager::all_reduce_sum
};

} // namespace op
```

---

#### 8.4.3 模型层改动：LLamaModel

**现有 MatMul 层替换规则**：

| 原有层 | 替换为 | 说明 |
|---|---|---|
| QKV MatMul | `ColumnParallelLinear` | 每卡算 `num_heads/P` 个头 |
| O Proj MatMul | `RowParallelLinear` | 输出 AllReduce |
| gate_proj / up_proj | `ColumnParallelLinear` | FFN 输入端 |
| down_proj | `RowParallelLinear` | FFN 输出端 + AllReduce |
| Embedding | 按 vocab 行切分（`VocabParallelEmbedding`）| AllGather |
| LM Head | 按 vocab 列切分 | AllReduce logits |

**关键**：RMSNorm、RoPE、SwiGLU 不需要改动（均为 element-wise 操作，在复制数据上本地执行）。

---

#### 8.4.4 KV Cache：每卡只存本卡 head 的 Cache

由于 QKV 已列并行切分，每卡只负责 `num_kv_heads / world_size` 个 KV head：

- `KVCacheManager::allocate_kv_cache()` 中 `num_kv_heads` 改为 `config.kv_head_num_ / world_size`
- 每卡 KV Cache 显存占用线性降低（`1/P`）
- PagedAttention kernel 无需修改（只操作本卡的 KV head 分片）

---

#### 8.4.5 权重切分：导出脚本修改

`tools/export_llama2.py` 增加 `--tp_size N` 参数，导出 N 份分片权重：

```
models/llama3_70b_tp2/
  rank0/llama3_fp32.bin   # W[:, :N/2]  的所有层
  rank1/llama3_fp32.bin   # W[:, N/2:]  的所有层
  tokenizer.bin
```

权重切分逻辑：
- **列并行层**：沿 axis=0（输出维度）切分
- **行并行层**：沿 axis=1（输入维度）切分
- **Embedding**：沿 vocab 维度切分
- **RMSNorm weights**：完整复制到每张卡（体积极小，无需切分）

---

#### 8.4.6 进程启动方式

使用 `torchrun` 或自定义 launcher 分别在每张 GPU 上启动一个进程：

```bash
# 2 卡并行推理
torchrun --nproc_per_node=2 --standalone \
    demo/llama3_tp2  --model llama3 --tp_size 2

# 或用 mpirun
mpirun -np 2 ./build/demo/llama3 --model llama3 --tp_size 2
```

**进程间同步**：
- 每个进程读取自己的分片权重（`rank0/llama3_fp32.bin`）
- Tokenize、Encode、Decode 仅在 `rank == 0` 的进程执行
- Forward pass 在所有进程上同步进行

---

### 8.5 详细实现步骤

#### Phase 1：基础通信层（约 2 天）

1. `CMakeLists.txt` 集成 NCCL：
   ```cmake
   find_package(NCCL REQUIRED)
   target_link_libraries(nanoinfer PRIVATE NCCL::NCCL)
   ```
2. 实现 `CommManager`：初始化 NCCL communicator、`all_reduce_sum`、`broadcast`
3. 新增 `base::init_distributed(rank, world_size)` 全局初始化接口
4. 单元测试：2 卡 AllReduce 正确性验证（`test/test_base/test_comm.cpp`）

#### Phase 2：并行 Linear 算子（约 2 天）

1. 实现 `ColumnParallelLinear`：本地 MatMul，无通信
2. 实现 `RowParallelLinear`：本地 MatMul + AllReduce
3. 实现 `VocabParallelEmbedding`：行切分 Embedding + AllReduce
4. 单元测试：2 卡并行 vs 单卡全量结果对比（数值误差 < 1e-5）

#### Phase 3：权重导出工具（约 1.5 天）

1. `tools/export_llama2.py` 增加 `--tp_size` 参数
2. 实现权重切分逻辑（列/行方向）
3. 每张卡导出独立的 `.bin` 文件
4. 验证：单卡 vs 合并后结果完全一致

#### Phase 4：模型层集成（约 2 天）

1. `LLamaModel::init_layers()` 中根据 `tp_size > 1` 决定使用并行还是普通 Linear
2. 新增 `TransformerConfig::tp_rank_` 和 `tp_size_` 字段
3. KV Cache 分配改为 `kv_head_num / tp_size`
4. Logits AllReduce（LM Head 输出后）

#### Phase 5：推理入口改造（约 1 天）

1. `demo/llama2.cpp` 增加 `--tp_size N` 参数
2. `main()` 开头调用 `base::init_distributed(rank, world_size)`
3. rank != 0 的进程不打印输出、不执行 tokenize
4. 端到端测试：2 卡推理结果与单卡完全一致

#### Phase 6：性能调优（约 2 天）

1. 通信 stream 与计算 stream 分离，overlap AllReduce 与下一层计算
2. NVLink 拓扑感知调度（优先使用 NVLink P2P 路径）
3. 压测：2/4/8 卡吞吐对比，计算 AllReduce 通信开销占比

---

### 8.6 关键文件变更清单

```
新增：
  include/nanoinfer/base/comm.h          ← CommManager / CommContext
  src/base/comm.cpp
  include/nanoinfer/op/parallel_linear.h ← ColumnParallelLinear / RowParallelLinear
  src/op/parallel_linear.cpp
  test/test_base/test_comm.cpp
  test/test_op/test_parallel_linear.cpp

修改：
  CMakeLists.txt                         ← 链接 NCCL
  include/nanoinfer/model/config.h       ← 增加 tp_rank_, tp_size_
  include/nanoinfer/model/llama.h        ← init_layers 支持并行层
  src/model/llama.cpp                    ← 层初始化切换逻辑
  src/engine/kv_cache_manager.cpp        ← kv_head_num /= tp_size
  tools/export_llama2.py                 ← --tp_size 权重切分
  demo/llama2.cpp                        ← --tp_size 参数 + init_distributed
  demo/batched_infer_multi_prompts.cpp   ← 同上
```

---

### 8.7 正确性验证

```bash
# Step 1：导出 2 卡分片权重
python tools/export_llama2.py --model_dir ./hf/llama3 --tp_size 2 --outdir ./models/llama3_tp2

# Step 2：单卡推理（基准）
./build/demo/llama3 --model llama3 --tp_size 1 > output_tp1.txt

# Step 3：2 卡推理
mpirun -np 2 ./build/demo/llama3 --model llama3 --tp_size 2 > output_tp2.txt

# Step 4：对比（greedy 推理下结果应完全一致）
diff output_tp1.txt output_tp2.txt
```

同时用 `eval/hf_verify.py` 作为精度基准进行端到端对比。

---

### 8.8 长期规划：Pipeline Parallelism（预留）

当卡数进一步增加（P > 8）或模型极深时，TP 的 AllReduce 通信开销会成瓶颈，此时考虑引入 PP：

- 将 N 个 Transformer 层分成 P 组，每组放一张卡
- 相邻卡之间传输激活（P2P `cudaMemcpyPeerAsync`）
- 需要 micro-batch 流水线调度消除气泡（GPipe / PipeDream 方案）
- `InferenceRequest` 需支持跨卡的激活暂存与恢复

PP 实现复杂度显著高于 TP，建议 TP 稳定后再行规划。

---

### 8.9 预计工作量汇总

| Phase | 内容 | 工作量 |
|---|---|---|
| Phase 1 | NCCL 通信基础层 | 2 天 |
| Phase 2 | 并行 Linear 算子 | 2 天 |
| Phase 3 | 权重切分导出工具 | 1.5 天 |
| Phase 4 | 模型层集成 | 2 天 |
| Phase 5 | 推理入口改造 | 1 天 |
| Phase 6 | 性能调优 & 压测 | 2 天 |
| **合计** | | **~10.5 天** |

---

## 九、优先级与时间线建议

| 优先级 | 任务 | 预计工作量 | 依赖 |
|--------|------|-----------|------|
| P0 (高) | **CPU: 集成 OpenBLAS MatMul** | 0.5-1 天 | 无 |
| P0 (高) | **CPU: Attention heads OpenMP 并行** | 0.5 天 | 无 |
| P0 (高) | 预分配 workspace 替代 cudaMallocAsync | 1 天 | 无 |
| P0 (高) | CUDA 错误检查宏 | 0.5 天 | 无 |
| P1 (中) | **CPU: Prefill cblas_sgemm 批量 attention** | 1 天 | OpenBLAS |
| P1 (中) | **CPU: 权重 INT8 量化** | 2-3 天 | 无 |
| P1 (中) | 采样策略 (Temperature + Top-P) | 2-3 天 | 无 |
| P1 (中) | 非最后 chunk 跳过 LM Head | 1 天 | 无 |
| P1 (中) | FP16 推理 | 3-5 天 | 无 |
| P2 | **CPU: mmap 权重加载** | 0.5 天 | 无 |
| P2 | **CPU: 算子融合 (SwiGLU/RMSNorm)** | 1-2 天 | 无 |
| P2 | Qwen 模型支持 | 3-5 天 | FP16 (可选) |
| P2 | Priority 调度 | 1-2 天 | 无 |
| P2 | Chunked Prefill 单元测试 | 1 天 | 无 |
| P3 (低) | **CPU: 手写 AVX2 micro-kernel** | 3-5 天 | 无 |
| P3 (低) | FlashAttention 集成 | 5-7 天 | FP16 |
| P3 (低) | CUDA Graph | 3-5 天 | 无 |
| P3 (低) | Speculative Decoding | 5-7 天 | 多模型加载 |
| P3 (低) | Priority + Preemption (KV Swap) | 3-5 天 | Priority 调度 |
| P3 (低) | **多卡 TP：NCCL 通信层** | 2 天 | 无 |
| P3 (低) | **多卡 TP：并行 Linear 算子** | 2 天 | NCCL 通信层 |
| P3 (低) | **多卡 TP：权重切分导出 + 模型集成** | 3.5 天 | 并行算子 |
| P3 (低) | **多卡 TP：推理入口 + 端到端验证** | 3 天 | 模型集成 |
