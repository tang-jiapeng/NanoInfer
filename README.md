# ğŸ”¬ NanoInfer

<p align="center">
  <b>A lightweight LLM inference engine built from scratch in C++/CUDA</b><br>
  <i>PagedAttention Â· Continuous Batching Â· Chunked Prefill Â· Prefix Caching Â· Configurable Sampling</i>
</p>

---

## ğŸ“– Overview

NanoInfer is a minimal yet functional inference framework designed to explore and implement core techniques used in modern LLM serving systems. It supports end-to-end inference from model loading to text generation, with a focus on memory-efficient KV cache management, high-throughput batched decoding, and flexible sampling strategies.

---

## âœ¨ Features

### ğŸ¤– Model Support

| Model | Backend | FP32 | W8A32 INT8 |
|-------|:-------:|:----:|:----------:|
| TinyLlama (LLaMA 2) | CPU | âœ… | âŒ |
| | GPU | âœ… | âœ… |
| LLaMA 3.2 | CPU | âœ… | âŒ |
| | GPU | âœ… | âœ… |
| Qwen3 0.6B | CPU | âœ… | âŒ |
| | GPU | âœ… | âŒ |

> ğŸ“¦ Unified export tooling: `tools/export_models.sh` â€” download from HuggingFace â†’ convert to custom binary format

### âš¡ Inference Engine

| Feature | Description |
|---------|-------------|
| ğŸ”„ **Continuous Batching** | Dynamic request scheduling with concurrent prefill and decode |
| ğŸ“„ **PagedAttention** | vLLM-style block-based KV cache â€” Block Manager, Block Table (logical â†’ physical), per-layer paged K/V |
| ğŸ§© **Chunked Prefill** | Fixed-size chunks (default 512), O(chunk Ã— ctx) instead of O(seqÂ²), prevents OOM on long prompts |
| ğŸ—‚ï¸ **Prefix Caching** | Hash-based block deduplication â€” reuse KV cache across multi-turn conversations and shared-prefix workloads |
| ğŸ“‹ **Scheduler** | FCFS policy, configurable max batch size / max sequences / prefill chunk size |

### ğŸ² Configurable Sampler (vLLM-style)

Per-request sampling parameters with a **fused CUDA kernel** pipeline:

```
RepetitionPenalty â†’ Temperature â†’ Top-K â†’ Top-P (Nucleus) â†’ Softmax â†’ Multinomial
```

| Parameter | Description | Default |
|-----------|-------------|:-------:|
| `temperature` | Controls randomness (0 = greedy argmax) | `1.0` |
| `top_k` | Keep top-K highest probability tokens (-1 = disabled) | `-1` |
| `top_p` | Nucleus sampling threshold (1.0 = disabled) | `1.0` |
| `repetition_penalty` | Penalize previously generated tokens (1.0 = disabled) | `1.0` |
| `seed` | Random seed for reproducibility (-1 = random) | `-1` |

### ğŸ”§ CUDA & CPU Kernels

> All operators have **both CUDA and CPU** implementations for dual-device support.

| Kernel | Description |
|--------|-------------|
| Embedding | Token ID â†’ embedding vector lookup |
| RMSNorm | Root Mean Square Layer Normalization |
| MatMul | cuBLAS-based matrix multiplication (batched) |
| RoPE | Rotary Positional Embedding (LLaMA 3.2 scaling) |
| SwiGLU | SwiGLU activation for FFN |
| PagedAttention | Decode-phase attention with paged KV cache |
| Prefill Attention | Gather paged K/V â†’ cuBLAS GEMM â†’ chunked causal softmax |
| Paged KV Write | Write K/V into block-based cache |
| KV Cache Gather | Collect scattered K/V into contiguous buffer |
| Add | Residual connection (element-wise) |
| Sampling | Fused Rep-Penalty / Temp / Top-K / Top-P / Softmax / Multinomial |
| Argmax | Batched greedy decoding fast path |

### ğŸ—ï¸ Architecture

```
Embedding â†’ [ RMSNorm â†’ QKV â†’ RoPE â†’ PagedAttn â†’ Wo â†’ Add â†’ RMSNorm â†’ FFN(SwiGLU) â†’ Add ] Ã— N â†’ RMSNorm â†’ Linear â†’ Sampler
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Engine                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Scheduler  â”‚  â”‚ Model â”‚  â”‚   Sampler   â”‚ â”‚
â”‚  â”‚ (FCFS)     â”‚  â”‚(LLaMA)â”‚  â”‚(Configurableâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚        â”‚            â”‚              â”‚         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚          KV Cache Manager              â”‚ â”‚
â”‚  â”‚  (Block Manager + Prefix Caching)      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
NanoInfer/
â”œâ”€â”€ include/nanoinfer/       # Public headers
â”‚   â”œâ”€â”€ base/                # Allocators, device config, utilities
â”‚   â”œâ”€â”€ engine/              # Engine, Scheduler, KVCacheManager, BlockTable
â”‚   â”œâ”€â”€ model/               # Model config, LLaMA implementation
â”‚   â”œâ”€â”€ op/                  # Operator layer interfaces
â”‚   â”œâ”€â”€ sampler/             # ConfigurableSampler, SamplingParams
â”‚   â””â”€â”€ tensor/              # Tensor abstraction
â”œâ”€â”€ src/                     # Implementation
â”‚   â”œâ”€â”€ op/kernels/cuda/     # CUDA kernel implementations
â”‚   â””â”€â”€ op/kernels/cpu/      # CPU kernel implementations
â”œâ”€â”€ demo/                    # Inference demos
â”‚   â”œâ”€â”€ chat_demo.cpp        # ğŸ’¬ Interactive multi-turn chat (streaming)
â”‚   â”œâ”€â”€ sampling_strategies_demo.cpp  # ğŸ² Sampling strategy comparison
â”‚   â”œâ”€â”€ batched_infer_multi_prompts.cpp  # ğŸ”„ Multi-prompt continuous batching
â”‚   â”œâ”€â”€ prefix_caching_benchmark.cpp    # ğŸ—‚ï¸ Prefix caching performance
â”‚   â””â”€â”€ ...                  # Additional demos (CPU, single-prompt)
â”œâ”€â”€ test/                    # âœ… Unit tests (GTest)
â”‚   â”œâ”€â”€ test_cuda_kernel/    # Per-kernel correctness tests
â”‚   â”œâ”€â”€ test_engine/         # Engine, scheduler, sampling, prefix caching
â”‚   â”œâ”€â”€ test_op/             # Operator layer tests
â”‚   â””â”€â”€ test_base/           # Allocator, tensor, buffer tests
â”œâ”€â”€ eval/                    # ğŸ“Š Accuracy verification (HuggingFace comparison)
â”œâ”€â”€ tools/                   # ğŸ› ï¸ Model export & management scripts
â”‚   â”œâ”€â”€ export_models.sh     # Unified download + export
â”‚   â”œâ”€â”€ export_llama2.py     # LLaMA 2 weight converter
â”‚   â””â”€â”€ export_llama3.py     # LLaMA 3 weight converter
â”œâ”€â”€ third_party/tiktoken/    # tiktoken BPE tokenizer (LLaMA 3)
â””â”€â”€ cmake/                   # CMake modules (CPM, CUDA config)
```

---

## ğŸ”¨ Build

### Prerequisites

- CMake â‰¥ 3.16
- CUDA Toolkit (tested with CUDA 11.x / 12.x)
- C++17 compiler (GCC / Clang)

Dependencies managed automatically via [CPM.cmake](https://github.com/cpm-cmake/CPM.cmake):

| Dependency | Purpose |
|------------|---------|
| [glog](https://github.com/google/glog) | Logging |
| [Google Test](https://github.com/google/googletest) | Testing |
| [SentencePiece](https://github.com/google/sentencepiece) | LLaMA 2 tokenizer |
| [Armadillo](https://arma.sourceforge.net/) | CPU linear algebra |
| [nlohmann/json](https://github.com/nlohmann/json) | JSON parsing |
| [re2](https://github.com/google/re2) | Regex (tiktoken) |
| [abseil-cpp](https://github.com/abseil/abseil-cpp) | Utilities |

### Compile

```bash
mkdir build && cd build
cmake ..
make -j$(nproc)
```

---

## ğŸš€ Usage

### 1. ğŸ“¦ Export Models

```bash
# Download and export all supported models
bash tools/export_models.sh all

# Or export individually:
bash tools/export_models.sh download-llama3-instruct
bash tools/export_models.sh export-llama3-instruct-fp32
```

### 2. ğŸ’¬ Interactive Chat (LLaMA 3.2 1B Instruct)

```bash
./build/demo/chat_demo --model llama3
```

Multi-turn conversation with streaming token output, prefix caching, and configurable sampling (`temp=0.7, top_k=40, top_p=0.9`).

### 3. ğŸ² Sampling Strategies Demo

```bash
./build/demo/sampling_strategies_demo --model llama3
```

Side-by-side comparison of Greedy, Temperature, Top-K, Top-P, and combined sampling strategies.

### 4. ğŸ”„ Multi-Prompt Batched Inference

```bash
./build/demo/batched_infer_multi_prompts --model llama3
```

Continuous Batching with multiple prompts of varying lengths â€” parallel prefill + batched decode.

### 5. âœ… Run Tests

```bash
cd build && ctest --output-on-failure
```

---

## ğŸ“Š Accuracy Verification

Compare NanoInfer outputs against HuggingFace transformers token-by-token:

```bash
pip install -r eval/requirements.txt
python eval/hf_verify.py --model_dir ./models/tinyllama_hf
```

See [eval/README.md](eval/README.md) for details.

---

## ğŸ™ Acknowledgements

- The initial inspiration and reference implementation provided by [KuiperLLama](https://github.com/zjhellofss/KuiperLLama).
- Generative AI tools (Gemini, Claude Code) were extensively used for code review, debugging, and optimization.