#include "nanoinfer/model/llama.h"
#include <nanoinfer/op/matmul.h>
#include <nanoinfer/op/rmsnorm.h>
#include "../op/kernels/cpu/rope_kernel.h"
#include "nanoinfer/op/add.h"
#include "nanoinfer/op/mha.h"
#include "nanoinfer/op/rope.h"
#include "nanoinfer/op/swiglu.h"

namespace model {

void LLamaLayers::to_cuda(std::shared_ptr<kernel::CudaConfig> config) {
    if (add_layer_) {
        add_layer_->set_cuda_config(config);
        add_layer_->to_cuda();
    }

    if (rope_layer_) {
        rope_layer_->set_cuda_config(config);
        rope_layer_->to_cuda();
    }

    if (swiglu_layer_) {
        swiglu_layer_->set_cuda_config(config);
        swiglu_layer_->to_cuda();
    }

    if (cls_layer_) {
        cls_layer_->set_cuda_config(config);
        cls_layer_->to_cuda();
    }

    if (embedding_layer_) {
        embedding_layer_->set_cuda_config(config);
        embedding_layer_->to_cuda();
    }

    if (mha_layer_) {
        mha_layer_->set_cuda_config(config);
        mha_layer_->to_cuda();
    }

    for (auto& weight_layer : wq_layers_) {
        if (weight_layer) {
            weight_layer->set_cuda_config(config);
            weight_layer->to_cuda();
        }
    }

    for (auto& weight_layer : wk_layers_) {
        if (weight_layer) {
            weight_layer->set_cuda_config(config);
            weight_layer->to_cuda();
        }
    }

    for (auto& weight_layer : wv_layers_) {
        if (weight_layer) {
            weight_layer->set_cuda_config(config);
            weight_layer->to_cuda();
        }
    }

    for (auto& weight_layer : wo_layers_) {
        if (weight_layer) {
            weight_layer->set_cuda_config(config);
            weight_layer->to_cuda();
        }
    }

    for (auto& weight_layer : w1_layers_) {
        if (weight_layer) {
            weight_layer->set_cuda_config(config);
            weight_layer->to_cuda();
        }
    }

    for (auto& weight_layer : w2_layers_) {
        if (weight_layer) {
            weight_layer->set_cuda_config(config);
            weight_layer->to_cuda();
        }
    }

    for (auto& weight_layer : w3_layers_) {
        if (weight_layer) {
            weight_layer->set_cuda_config(config);
            weight_layer->to_cuda();
        }
    }

    for (auto& rms_norm_layer : rmsnorm_layers_) {
        if (rms_norm_layer) {
            rms_norm_layer->to_cuda();
            rms_norm_layer->set_cuda_config(config);
        }
    }
}

LLamaModel::LLamaModel(base::TokenizerType tokenizer_type, std::string token_path,
                       std::string model_path, bool is_quant_model)
    : Model(tokenizer_type, base::ModelType::kModelTypeLLama2, std::move(token_path),
            std::move(model_path), is_quant_model) {
}

/**
 * @brief Llama 模型初始化流程
 * 1. 检查路径和设备兼容性。
 * 2. 初始化 CUDA 上下文 (如果需要)。
 * 3. 加载模型文件 (mmap)。
 * 4. 分配显存 (init_mem)。
 * 5. 预计算 RoPE 的 Sin/Cos 缓存表。
 * 6. 初始化采样器。
 */
base::Status LLamaModel::init(base::DeviceType device_type) {
    if (token_path_.empty()) {
        return base::error::PathNotValid(token_path_);
    }
    // CPU 暂不支持 Int8 量化推理
    if (device_type == base::DeviceType::kDeviceCPU && is_quant_model_) {
        return base::error::InternalError(
            "The cpu device do not support int8 quant model.");
    }

    device_type_ = device_type;
    // [CUDA Init]: 创建 Stream 句柄
    if (device_type == base::DeviceType::kDeviceCUDA) {
        cudaSetDevice(0);
        cuda_config_ = std::make_shared<kernel::CudaConfig>();
        cudaStreamCreate(&cuda_config_->stream);
        cudaError_t err = cudaGetLastError();
        if (err != cudaSuccess) {
            return base::error::InternalError("The cuda handle create failed.");
        }
    }

    // [Model Loading]: 从文件映射权重并构建层
    base::Status read_status = gen_model_from_file();
    if (!read_status) {
        return read_status;
    }

    // [Memory Alloc]: 预分配 KV Cache 和中间 Buffer
    init_mem();

    // [Pre-calc RoPE]: 计算旋转位置编码的 Sin/Cos 表
    if (device_type_ == base::DeviceType::kDeviceCPU) {
        kernel::sin_cos_cache_calc_cpu(
            config_->head_size_, config_->seq_len_,
            get_buffer(ModelBufferType::kSinCache).ptr<float>(),
            get_buffer(ModelBufferType::kCosCache).ptr<float>());
    } else {
        // CHECK_NE(cuda_config_, nullptr);
        // kernel::sin_cos_cache_calc_cu(config_->head_size_, config_->seq_len_,
        //                               get_buffer(ModelBufferType::kSinCache),
        //                               get_buffer(ModelBufferType::kCosCache),
        //                               cuda_config_->stream);
    }

    // [Sampler Init]: 默认使用 Argmax 采样
    sampler_ = std::make_unique<sampler::ArgmaxSampler>(device_type_);
    return base::error::Success();
}

/**
 * @brief 模型前向传播主循环 (The Main Loop)
 *
 * 对应 Transformer 的 Decoder 架构：
 * Input -> [Block 0] -> [Block 1] ... -> [Block N] -> Output
 */
base::Status LLamaModel::forward(const tensor::Tensor& input,
                                 const tensor::Tensor& pos_tensor, int& next) const {
    if (input.is_empty()) {
        return base::error::InvalidArgument("The input tensor is empty.");
    }
    if (device_type_ == base::DeviceType::kDeviceCPU && is_quant_model_) {
        return base::error::InternalError("Unsupported int8 quant in the cpu device");
    }

    // 逐层执行 Transformer Block
    for (int32_t layer_idx = 0; layer_idx < config_->layer_num_; ++layer_idx) {
        // RMSNorm (Pre-Attention)
        attention_rms(layer_idx, input);
        // QKV Projection & RoPE
        attention_qkv(layer_idx, pos_tensor);
        // Multi-Head Attention (Self-Attention)
        attention_mha(layer_idx, pos_tensor);
        // Feed Forward Network (FFN) with Residual
        feed_forward(layer_idx, input);
    }
    // Final Classification (Norm -> Linear)
    cls_logits(input);
    return base::error::Success();
}

/**
 * @brief 创建无参数层 (Stateless Layers)
 * 这些层在所有 Block 间共享，或者没有权重状态。
 */
void LLamaModel::create_nonparam_layers() {
    CHECK(llama_layers_ != nullptr);
    // RoPE 层：处理位置编码
    llama_layers_->rope_layer_ = std::make_shared<op::RoPELayer>(
        device_type_, config_->dim_, config_->kv_dim_, config_->head_size_);

    // MHA 层：处理 Attention 核心计算 (Softmax, Score)
    llama_layers_->mha_layer_ = std::make_shared<op::MultiHeadAttention>(
        device_type_, 0, config_->kv_mul_, config_->kv_dim_, config_->seq_len_,
        config_->head_num_, config_->head_size_);

    // Add 层：处理残差连接
    llama_layers_->add_layer_ = std::make_shared<op::VecAddLayer>(device_type_);

    // SwiGLU 层：FFN 的激活函数
    llama_layers_->swiglu_layer_ =
        std::make_shared<op::SwiGLULayer>(device_type_, config_->hidden_dim_);
}

/**
 * @brief 创建并加载量化参数层 (Int8 Weights)
 * 这里的加载逻辑假设模型文件是按 "参数类型分组" (Grouped by Parameter Type)存储的。
 * 即：先存所有层的 Wq，再存所有层的 Wk... 而不是按 Layer 0 (Wq, Wk...), Layer 1...
 */
void LLamaModel::create_param_quant_layers() {
    CHECK(is_quant_model_);
    CHECK(llama_layers_ != nullptr);

    size_t pos = 0;  // 文件指针偏移量
    int32_t dim = config_->dim_;
    auto cpu_device_type = base::DeviceType::kDeviceCPU;

    // 加载所有层的 Query Weights (Wq)
    for (int32_t i = 0; i < config_->layer_num_; ++i) {
        auto wq = std::make_shared<op::MatmulLayer>(device_type_, dim, dim, true);
        wq->set_group_size(group_size_);
        wq->set_weight(0, {dim, dim}, this->raw_model_data_->weight(pos),
                       cpu_device_type);
        llama_layers_->wq_layers_.push_back(wq);
        pos = pos + dim * dim + wq->get_scale_num() * sizeof(float);
    }

    // 加载所有层的 Key Weights (Wk)
    for (int32_t i = 0; i < config_->layer_num_; ++i) {
        auto wk =
            std::make_shared<op::MatmulLayer>(device_type_, config_->kv_dim_, dim, true);
        wk->set_group_size(group_size_);
        wk->set_weight(0, {config_->kv_dim_, dim}, this->raw_model_data_->weight(pos),
                       cpu_device_type);
        llama_layers_->wk_layers_.push_back(wk);
        pos = pos + config_->kv_dim_ * dim + wk->get_scale_num() * sizeof(float);
    }

    // 加载所有层的 Value Weights (Wv)
    for (int32_t i = 0; i < config_->layer_num_; ++i) {
        auto wv =
            std::make_shared<op::MatmulLayer>(device_type_, config_->kv_dim_, dim, true);
        wv->set_group_size(group_size_);
        wv->set_weight(0, {config_->kv_dim_, dim}, this->raw_model_data_->weight(pos),
                       cpu_device_type);
        llama_layers_->wv_layers_.push_back(wv);
        pos += config_->kv_dim_ * dim + wv->get_scale_num() * sizeof(float);
    }

    // 加载所有层的 Output Weights (Wo)
    for (int32_t i = 0; i < config_->layer_num_; ++i) {
        auto wo = std::make_shared<op::MatmulLayer>(device_type_, dim, dim, true);
        wo->set_group_size(group_size_);
        wo->set_weight(0, {dim, dim}, this->raw_model_data_->weight(pos),
                       cpu_device_type);
        llama_layers_->wo_layers_.push_back(wo);
        pos = pos + dim * dim + wo->get_scale_num() * sizeof(float);
    }

    // 加载 FFN W1 (Gate)
    int32_t hidden_dim = config_->hidden_dim_;
    for (int32_t i = 0; i < config_->layer_num_; ++i) {
        auto w1 = std::make_shared<op::MatmulLayer>(device_type_, hidden_dim, dim, true);
        w1->set_group_size(group_size_);
        w1->set_weight(0, {hidden_dim, dim}, this->raw_model_data_->weight(pos),
                       cpu_device_type);
        llama_layers_->w1_layers_.push_back(w1);
        pos = pos + dim * hidden_dim + w1->get_scale_num() * sizeof(float);
    }

    // 加载 FFN W2 (Down)
    for (int32_t i = 0; i < config_->layer_num_; ++i) {
        auto w2 = std::make_shared<op::MatmulLayer>(device_type_, dim, hidden_dim, true);
        w2->set_group_size(group_size_);
        w2->set_weight(0, {dim, hidden_dim}, this->raw_model_data_->weight(pos),
                       cpu_device_type);
        llama_layers_->w2_layers_.push_back(w2);
        pos = pos + dim * hidden_dim + w2->get_scale_num() * sizeof(float);
    }

    // 加载 FFN W3 (Up)
    for (int32_t i = 0; i < config_->layer_num_; ++i) {
        auto w3 = std::make_shared<op::MatmulLayer>(device_type_, hidden_dim, dim, true);
        w3->set_group_size(group_size_);
        w3->set_weight(0, {hidden_dim, dim}, this->raw_model_data_->weight(pos),
                       cpu_device_type);
        llama_layers_->w3_layers_.push_back(w3);
        pos = pos + dim * hidden_dim + w3->get_scale_num() * sizeof(float);
    }

    // 加载 Final Classification Layer (Cls)
    auto cls_layer =
        std::make_shared<op::MatmulLayer>(device_type_, config_->vocab_size_, dim, true);
    cls_layer->set_group_size(group_size_);
    // 处理 Shared Weight (Embedding 与 Output 权重共享的情况)
    if (config_->is_shared_weight_) {
        // // 复用 Embedding 权重 (位于 offset 0)
        cls_layer->set_weight(0, {config_->vocab_size_, dim},
                              this->raw_model_data_->weight(pos), cpu_device_type);
    } else {
        // no shared
        cls_layer->set_weight(0, {config_->vocab_size_, dim},
                              this->raw_model_data_->weight(pos), cpu_device_type);
        pos =
            pos + config_->vocab_size_ * dim + cls_layer->get_scale_num() * sizeof(float);
    }
    llama_layers_->cls_layer_ = cls_layer;

    // 加载 Embedding Layer
    // Embedding 始终为 FP32 (float*)，即使在量化模型中通常也不量化 Embedding
    float* weight_ptr = (float*)raw_model_data_->weight(pos);
    llama_layers_->embedding_layer_ = std::make_shared<op::EmbeddingLayer>(
        device_type_, config_->dim_, config_->seq_len_, std::abs(config_->vocab_size_));
    llama_layers_->embedding_layer_->set_weight(0, {std::abs(config_->vocab_size_), dim},
                                                weight_ptr, cpu_device_type);
    weight_ptr += config_->vocab_size_ * dim;

    // 加载 RMSNorm 权重 (Attention Norm, FFN Norm, Final Norm)
    // 这里的循环逻辑是将所有 Norm 层统一加载到 rmsnorm_layers_ 中
    // 包含：[AttnNorm * LayerNum, FFN Norm * LayerNum, Final Norm]
    for (int32_t i = 0; i < 2 * config_->layer_num_ + 1; ++i) {
        std::shared_ptr<op::RmsNormLayer> rms_norm_layer =
            std::make_shared<op::RmsNormLayer>(device_type_, dim);

        rms_norm_layer->set_weight(0, {dim}, weight_ptr, cpu_device_type);
        llama_layers_->rmsnorm_layers_.push_back(rms_norm_layer);
        weight_ptr += dim;
    }
}

/**
 * @brief 创建并加载浮点参数层 (FP32 Weights)
 * 逻辑与 create_param_quant_layers 类似，但不需要处理 Scales 和 Group Size。
 */
void LLamaModel::create_param_layers() {
    CHECK(!is_quant_model_);
    CHECK(llama_layers_ != nullptr);
    // 加载 Embedding (位于文件开头 0 偏移)
    auto cpu_device_type = base::DeviceType::kDeviceCPU;
    llama_layers_->embedding_layer_ = std::make_shared<op::EmbeddingLayer>(
        device_type_, config_->dim_, config_->seq_len_, std::abs(config_->vocab_size_));

    const void* weight_embedding = raw_model_data_->weight(0);
    llama_layers_->embedding_layer_->set_weight(
        0, {std::abs(config_->vocab_size_), config_->dim_}, weight_embedding,
        cpu_device_type);

    // 加载 Wq, Wk, Wv, Wo, W1, W2, W3... (按类型分组)
    int32_t dim = config_->dim_;
    size_t pos = dim * std::abs(config_->vocab_size_) + dim * config_->layer_num_;
    // create weight matrix for query
    for (int32_t i = 0; i < config_->layer_num_; ++i) {
        auto wq = std::make_shared<op::MatmulLayer>(device_type_, dim, dim);
        wq->set_weight(0, {dim, dim}, this->raw_model_data_->weight(pos),
                       cpu_device_type);
        llama_layers_->wq_layers_.push_back(wq);
        pos += dim * dim;
    }

    // create weight matrix for key
    for (int32_t i = 0; i < config_->layer_num_; ++i) {
        auto wk = std::make_shared<op::MatmulLayer>(device_type_, config_->kv_dim_, dim);
        wk->set_weight(0, {config_->kv_dim_, dim}, this->raw_model_data_->weight(pos),
                       cpu_device_type);
        llama_layers_->wk_layers_.push_back(wk);
        pos += config_->kv_dim_ * dim;
    }

    // create weight matrix for value
    for (int32_t i = 0; i < config_->layer_num_; ++i) {
        auto wv = std::make_shared<op::MatmulLayer>(device_type_, config_->kv_dim_, dim);
        wv->set_weight(0, {config_->kv_dim_, dim}, this->raw_model_data_->weight(pos),
                       cpu_device_type);
        llama_layers_->wv_layers_.push_back(wv);
        pos += config_->kv_dim_ * dim;
    }

    // create weight matrix for output
    for (int32_t i = 0; i < config_->layer_num_; ++i) {
        auto wo = std::make_shared<op::MatmulLayer>(device_type_, dim, dim);
        wo->set_weight(0, {dim, dim}, this->raw_model_data_->weight(pos),
                       cpu_device_type);
        llama_layers_->wo_layers_.push_back(wo);
        pos += dim * dim;
    }

    // skip ffn rmsnorm
    pos += config_->layer_num_ * dim;

    // w1 layers
    int32_t hidden_dim = config_->hidden_dim_;
    for (int32_t i = 0; i < config_->layer_num_; ++i) {
        auto w1 = std::make_shared<op::MatmulLayer>(device_type_, hidden_dim, dim);
        w1->set_weight(0, {hidden_dim, dim}, this->raw_model_data_->weight(pos),
                       cpu_device_type);
        llama_layers_->w1_layers_.push_back(w1);
        pos += dim * hidden_dim;
    }

    // w2 layers
    for (int32_t i = 0; i < config_->layer_num_; ++i) {
        auto w2 = std::make_shared<op::MatmulLayer>(device_type_, dim, hidden_dim);
        w2->set_weight(0, {dim, hidden_dim}, this->raw_model_data_->weight(pos),
                       cpu_device_type);
        llama_layers_->w2_layers_.push_back(w2);
        pos += dim * hidden_dim;
    }

    // w3 layers
    for (int32_t i = 0; i < config_->layer_num_; ++i) {
        auto w3 = std::make_shared<op::MatmulLayer>(device_type_, hidden_dim, dim);
        w3->set_weight(0, {hidden_dim, dim}, this->raw_model_data_->weight(pos),
                       cpu_device_type);
        llama_layers_->w3_layers_.push_back(w3);
        pos += dim * hidden_dim;
    }

    // skip final rms weight
    pos += dim;
    // skip freqs_cos and freqs_sin weight
    pos += config_->seq_len_ * config_->head_size_;

    llama_layers_->cls_layer_ =
        std::make_shared<op::MatmulLayer>(device_type_, config_->vocab_size_, dim);
    if (config_->is_shared_weight_) {
        // using token embedding weight
        llama_layers_->cls_layer_->set_weight(0, {config_->vocab_size_, dim},
                                              this->raw_model_data_->weight(0),
                                              cpu_device_type);
    } else {
        llama_layers_->cls_layer_->set_weight(0, {config_->vocab_size_, dim},
                                              this->raw_model_data_->weight(pos),
                                              cpu_device_type);
    }

    // create rmsnorm layer
    size_t rmsnorm_pos = config_->dim_ * std::abs(config_->vocab_size_);

    for (int32_t i = 0; i < config_->layer_num_; ++i) {
        std::shared_ptr<op::RmsNormLayer> rms_norm_layer =
            std::make_shared<op::RmsNormLayer>(device_type_, config_->dim_);

        const void* weight_rmsnorm = raw_model_data_->weight(rmsnorm_pos);
        rms_norm_layer->set_weight(0, {config_->dim_}, weight_rmsnorm, cpu_device_type);
        llama_layers_->rmsnorm_layers_.push_back(rms_norm_layer);
        rmsnorm_pos += config_->dim_;
    }

    // skip attention.wq attention.wk attention.wv attention.wo
    rmsnorm_pos += config_->layer_num_ * config_->dim_ * config_->dim_;
    rmsnorm_pos += config_->layer_num_ * config_->dim_ *
                   (config_->kv_head_num_ * config_->head_size_);
    rmsnorm_pos += config_->layer_num_ * config_->dim_ *
                   (config_->kv_head_num_ * config_->head_size_);
    rmsnorm_pos += config_->layer_num_ * config_->dim_ * config_->dim_;

    for (int32_t i = 0; i < config_->layer_num_; ++i) {
        std::shared_ptr<op::RmsNormLayer> rms_norm_layer =
            std::make_shared<op::RmsNormLayer>(device_type_, config_->dim_);
        const void* weight_rmsnorm = raw_model_data_->weight(rmsnorm_pos);
        rms_norm_layer->set_weight(0, {config_->dim_}, weight_rmsnorm, cpu_device_type);
        llama_layers_->rmsnorm_layers_.push_back(rms_norm_layer);

        rmsnorm_pos += config_->dim_;
    }

    // skip ffn.w1 ffn.w2 ffn.w3
    rmsnorm_pos += config_->layer_num_ * config_->hidden_dim_ * config_->dim_;
    rmsnorm_pos += config_->layer_num_ * config_->hidden_dim_ * config_->dim_;
    rmsnorm_pos += config_->layer_num_ * config_->hidden_dim_ * config_->dim_;

    std::shared_ptr<op::RmsNormLayer> rms_final_layer =
        std::make_shared<op::RmsNormLayer>(device_type_, config_->dim_);

    const void* weight_rmsnorm_final = raw_model_data_->weight(rmsnorm_pos);
    rms_final_layer->set_weight(0, {config_->dim_}, weight_rmsnorm_final,
                                cpu_device_type);
    llama_layers_->rmsnorm_layers_.push_back(rms_final_layer);
}

/**
 * @brief 显存预分配与初始化
 * * 这是一个关键函数。为了避免推理过程中频繁申请显存导致碎片和性能下降，
 * 我们预先计算好推理过程中所需的所有 Buffer，并一次性注册到 Model 基类的 buffers_
 * 映射中。
 * * 分配策略：
 * 1. 静态 Buffer (KV Cache, Sin/Cos Table): 生命周期贯穿整个模型运行期。
 * 2. 临时 Buffer (Intermediate Outputs): 用于层间传递数据，可以复用。
 */
void LLamaModel::init_mem() {
    std::shared_ptr<base::DeviceAllocator> alloc;
    if (device_type_ == base::DeviceType::kDeviceCPU) {
        alloc = base::CPUDeviceAllocatorFactory::get_instance();
    } else {
        alloc = base::CUDADeviceAllocatorFactory::get_instance();
    }

    // 确保所有 Layer 已获取 CUDA Config
    if (device_type_ == base::DeviceType::kDeviceCUDA) {
        CHECK_NE(cuda_config_, nullptr);
        llama_layers_->to_cuda(cuda_config_);
    }

    std::shared_ptr<base::DeviceAllocator> alloc_cpu =
        base::CPUDeviceAllocatorFactory::get_instance();
    std::shared_ptr<base::DeviceAllocator> alloc_cu =
        base::CUDADeviceAllocatorFactory::get_instance();

    // 输入输出 Buffer
    tensor::Tensor input_tokens(base::DataType::kDataTypeInt32, 1, true, alloc_cpu);
    tensor::Tensor input_embeddings(base::DataType::kDataTypeFp32, 1, config_->dim_, true,
                                    alloc);

    // RoPE 缓存表
    tensor::Tensor sin_cache(base::DataType::kDataTypeFp32,
                             config_->head_size_ * config_->seq_len_, true, alloc);
    tensor::Tensor cos_cache(base::DataType::kDataTypeFp32,
                             config_->head_size_ * config_->seq_len_, true, alloc);

    CHECK(insert_buffer(ModelBufferType::kSinCache, sin_cache));
    CHECK(insert_buffer(ModelBufferType::kCosCache, cos_cache));

    CHECK(insert_buffer(ModelBufferType::kInputTokens, input_tokens));
    CHECK(insert_buffer(ModelBufferType::kInputEmbeddings, input_embeddings));

    // 通用中间层输出 Buffer (可复用)
    // 用于存储 RMSNorm, MHA, FFN 等层的输出，通常 shape 为 [dim] 或 [hidden_dim]
    tensor::Tensor rms_output(base::DataType::kDataTypeFp32, config_->dim_, true, alloc);
    CHECK(insert_buffer(ModelBufferType::kOutputRMSNorm, rms_output));
    CHECK(insert_buffer(ModelBufferType::kOutputMHA, rms_output));
    CHECK(insert_buffer(ModelBufferType::kW2Output, rms_output));
    CHECK(insert_buffer(ModelBufferType::kFFNRMSNorm, rms_output));

    tensor::Tensor w1_output(base::DataType::kDataTypeFp32, config_->hidden_dim_, true,
                             alloc);
    tensor::Tensor w3_output(base::DataType::kDataTypeFp32, config_->hidden_dim_, true,
                             alloc);

    CHECK(insert_buffer(ModelBufferType::kW1Output, w1_output));
    CHECK(insert_buffer(ModelBufferType::kW3Output, w3_output));

    // KV Cache (占用显存最大)
    // Shape: [LayerNum, SeqLen, KVDim]
    tensor::Tensor key_cache(base::DataType::kDataTypeFp32, config_->layer_num_,
                             config_->seq_len_, config_->kv_dim_, true, alloc);
    tensor::Tensor value_cache(base::DataType::kDataTypeFp32, config_->layer_num_,
                               config_->seq_len_, config_->kv_dim_, true, alloc);

    CHECK(insert_buffer(ModelBufferType::kKeyCache, key_cache));
    CHECK(insert_buffer(ModelBufferType::kValueCache, value_cache));

    // Attention 中间变量
    tensor::Tensor query(base::DataType::kDataTypeFp32, config_->dim_, true, alloc);
    CHECK(insert_buffer(ModelBufferType::kQuery, query));

    tensor::Tensor pos_tensor(base::DataType::kDataTypeInt32, 1, true, alloc_cpu);
    CHECK(insert_buffer(ModelBufferType::kInputPos, pos_tensor));

    // 用于存储 Attention Score (Softmax 前后)
    tensor::Tensor attn(base::DataType::kDataTypeFp32, config_->head_num_,
                        config_->seq_len_, true, alloc);
    CHECK(insert_buffer(ModelBufferType::kScoreStorage, attn));
    CHECK(insert_buffer(ModelBufferType::kAttnOutput, query));

    // 最终输出 Logits
    tensor::Tensor forward_output(base::DataType::kDataTypeFp32, config_->vocab_size_,
                                  true, alloc);
    // 如果是 CUDA 模式，还需要一个 CPU 端的 Buffer 用于接收 Logits 以进行采样
    if (device_type_ == base::DeviceType::kDeviceCUDA) {
        tensor::Tensor forward_output_cpu(base::DataType::kDataTypeFp32,
                                          config_->vocab_size_, true, alloc_cpu);
        CHECK(insert_buffer(ModelBufferType::kForwardOutputCPU, forward_output_cpu));
    }

    CHECK(insert_buffer(ModelBufferType::kForwardOutput, forward_output));
}

base::Status LLamaModel::create_layers() {
    using namespace base;
    if (!llama_layers_) {
        llama_layers_ = std::make_unique<LLamaLayers>();
    }

    if (!is_quant_model_) {
        create_param_layers();
    } else {
        create_param_quant_layers();
    }
    create_nonparam_layers();

    if (!llama_layers_->embedding_layer_) {
        return error::InternalError(
            "Create the embedding layer for the llama model failed!");
    }

    // 校验 Norm 层数量 (2 * N + 1)
    if (llama_layers_->rmsnorm_layers_.size() != 2 * config_->layer_num_ + 1) {
        return error::InternalError(
            "Create the rmsnorm layers for the llama model failed!");
    }

    if (llama_layers_->wq_layers_.size() != config_->layer_num_ ||
        llama_layers_->wk_layers_.size() != config_->layer_num_ ||
        llama_layers_->wv_layers_.size() != config_->layer_num_ ||
        llama_layers_->wo_layers_.size() != config_->layer_num_) {
        return error::InternalError(
            "Create the matmul layer in the attention and ffn attention layers for the "
            "llama model "
            "failed.");
    }

    for (int32_t i = 0; i < config_->layer_num_; ++i) {
        if (!llama_layers_->wq_layers_.at(i) || !llama_layers_->wk_layers_.at(i) ||
            !llama_layers_->wv_layers_.at(i) || !llama_layers_->wo_layers_.at(i)) {
            return error::InternalError(
                "Create the matmul layer in the attention and ffn attention layers for "
                "the llama model "
                "failed.");
        }
    }

    if (llama_layers_->w1_layers_.size() != config_->layer_num_ ||
        llama_layers_->w2_layers_.size() != config_->layer_num_ ||
        llama_layers_->w3_layers_.size() != config_->layer_num_) {
        return error::InternalError(
            "Create the matmul layer in the feedforward layers for the llama model "
            "failed.");
    }

    for (int32_t i = 0; i < config_->layer_num_; ++i) {
        if (!llama_layers_->w1_layers_.at(i) || !llama_layers_->w2_layers_.at(i) ||
            !llama_layers_->w3_layers_.at(i)) {
            return error::InternalError(
                "Create the matmul layer in the feedforward layers for the llama model "
                "failed.");
        }
    }

    if (!llama_layers_->rope_layer_) {
        return error::InternalError("Create the rope layer for the llama model failed!");
    }

    if (!llama_layers_->add_layer_) {
        return error::InternalError("Create the add layer for the llama model failed!");
    }

    if (!llama_layers_->mha_layer_) {
        return error::InternalError("Create the mha layer for the llama model failed!");
    }

    if (!llama_layers_->swiglu_layer_) {
        return error::InternalError(
            "Create the SwiGLU layer for the llama model failed!");
    }
    return error::Success();
}

/**
 * @brief 执行 Embedding 查找
 * * 将离散的 Token IDs 转换为连续的 Vector，并处理 Buffer 的 Reshape 和赋值。
 */
op::EmbeddingOutput LLamaModel::embedding(const std::vector<int>& tokens) const {
    auto input_tokens = get_buffer(ModelBufferType::kInputTokens);
    auto input_embeddings = get_buffer(ModelBufferType::kInputEmbeddings);

    // 如果 Batch Size 变化，需要 Reshape (通常推理时 Batch=1)
    if (input_tokens.size() != tokens.size()) {
        input_tokens.reshape({static_cast<int32_t>(tokens.size())});
        input_embeddings.reshape({static_cast<int32_t>(tokens.size()), config_->dim_});
    }
    // 填充 Token ID 数据
    for (int32_t i = 0; i < tokens.size(); ++i) {
        input_tokens.index<int32_t>(i) = tokens.at(i);
    }

    auto input_token_num = tensor::Tensor(base::DataType::kDataTypeInt32,
                                          static_cast<int32_t>(tokens.size()));
    LOG_IF(FATAL, !llama_layers_->embedding_layer_)
        << "The embedding layer in the llama2 model is null pointer.";
    STATUS_CHECK(llama_layers_->embedding_layer_->forward(input_tokens, input_token_num,
                                                          input_embeddings));

    op::EmbeddingOutput output(input_tokens, input_embeddings, input_token_num);
    return output;
}

/**
 * @brief 执行 RMSNorm (Pre-Attention)
 * input -> RMSNorm -> kOutputRMSNorm
 */
void LLamaModel::attention_rms(int32_t layer_idx, const tensor::Tensor& input) const {
    CHECK(llama_layers_ != nullptr);
    // attn rmsnorm
    tensor::Tensor rmsnorm_output = get_buffer(ModelBufferType::kOutputRMSNorm);
    std::shared_ptr<op::Layer> rmsnorm_layer =
        llama_layers_->rmsnorm_layers_.at(layer_idx);
    if (!rmsnorm_layer) {
        LOG(FATAL) << "The attention rmsnorm layer is a null pointer in the llama2 model";
    }
    STATUS_CHECK(rmsnorm_layer->forward(input, rmsnorm_output));
}

/**
 * @brief 执行 QKV 投影和 RoPE
 * 1. 从 Cache 中获取当前层的 K, V 指针 (Zero-copy Slice)。
 * 2. 执行 Wq, Wk, Wv 投影。
 * 3. 执行 RoPE 旋转位置编码。
 */
void LLamaModel::attention_qkv(int32_t layer_idx,
                               const tensor::Tensor& pos_tensor) const {
    CHECK(llama_layers_ != nullptr);
    // kv cache
    tensor::Tensor query = this->get_buffer(ModelBufferType::kQuery);
    int32_t pos = pos_tensor.index<int32_t>(0);
    // [Slice KV Cache]: 获取当前层、当前位置的 KV Cache 写入位置
    const auto& [key, val] = slice_kv_cache(layer_idx, pos);
    // query
    const auto& query_layer = llama_layers_->wq_layers_.at(layer_idx);
    CHECK_NE(query_layer, nullptr)
        << "The query layer in the attention block is null pointer.";
    // [Projection]: Input * W -> Q/K/V
    auto rmsnorm_output = get_buffer(ModelBufferType::kOutputRMSNorm);
    STATUS_CHECK(query_layer->forward(rmsnorm_output, query));

    // key
    const auto& key_layer = llama_layers_->wk_layers_.at(layer_idx);
    CHECK_NE(key_layer, nullptr)
        << "The key layer in the attention block is null pointer.";
    STATUS_CHECK(key_layer->forward(rmsnorm_output, key));
    // value
    const auto& value_layer = llama_layers_->wv_layers_.at(layer_idx);
    CHECK_NE(value_layer, nullptr)
        << "The value layer in the attention block is null pointer.";
    STATUS_CHECK(value_layer->forward(rmsnorm_output, val));

    // [RoPE]: 对 Q 和 K 进行旋转
    CHECK_NE(llama_layers_->rope_layer_, nullptr)
        << "The RoPE layer in the attention block is null pointer.";
    STATUS_CHECK(llama_layers_->rope_layer_->forward(
        query, key, pos_tensor, get_buffer(ModelBufferType::kSinCache),
        get_buffer(ModelBufferType::kCosCache), tensor::Tensor{}));
}

base::Status LLamaModel::predict(const tensor::Tensor& input,
                                 const tensor::Tensor& pos_tensor, bool is_prompt,
                                 int& next) const {
    auto status = forward(input, pos_tensor, next);
    if (!status) {
        return status;
    }
    next = post_processing(pos_tensor, is_prompt);
    return base::error::Success();
}

/**
 * @brief 执行 Multi-Head Attention
 * Q, K, V -> MHA Kernel -> Output
 */
void LLamaModel::attention_mha(int32_t layer_idx,
                               const tensor::Tensor& pos_tensor) const {
    CHECK(llama_layers_ != nullptr);
    // mha
    tensor::Tensor key_cache = get_buffer(ModelBufferType::kKeyCache);
    // VAL = [val1,val2,...val t]
    // output @ VAL = 最终的结果
    tensor::Tensor val_cache = get_buffer(ModelBufferType::kValueCache);

    tensor::Tensor mha_output = get_buffer(ModelBufferType::kOutputMHA);
    tensor::Tensor score_storage = get_buffer(ModelBufferType::kScoreStorage);
    tensor::Tensor query = this->get_buffer(ModelBufferType::kQuery);

    const auto& mha_layer = llama_layers_->mha_layer_;
    CHECK_NE(mha_layer, nullptr) << "The multi head attention layer is null pointer.";
    int pos = pos_tensor.index<int32_t>(0);
    std::dynamic_pointer_cast<op::MultiHeadAttention>(mha_layer)->set_pos(pos);
    std::dynamic_pointer_cast<op::MultiHeadAttention>(mha_layer)->set_layer_idx(
        layer_idx);
    STATUS_CHECK(
        mha_layer->forward(query, score_storage, key_cache, val_cache, mha_output));

    // wo @ attention output
    tensor::Tensor attn_output = get_buffer(ModelBufferType::kAttnOutput);
    const auto& wo_layer = llama_layers_->wo_layers_.at(layer_idx);
    CHECK_NE(wo_layer, nullptr) << "The weight output layer is null pointer.";
    STATUS_CHECK(wo_layer->forward(mha_output, attn_output));
}

/**
 * @brief 执行 FFN 模块
 * Residual + Norm -> Gate/Up -> SwiGLU -> Down -> Residual Add
 */
void LLamaModel::feed_forward(int32_t layer_idx, const tensor::Tensor& input) const {
    CHECK(llama_layers_ != nullptr);
    // residual add
    CHECK_NE(llama_layers_->add_layer_, nullptr)
        << "The add layer in the feedforward block is null pointer";
    STATUS_CHECK(llama_layers_->add_layer_->forward(
        input, get_buffer(ModelBufferType::kAttnOutput), input));

    // ffn rmsnorm
    tensor::Tensor ffn_norm_output = get_buffer(ModelBufferType::kFFNRMSNorm);
    const auto& ffn_rmsnorm =
        llama_layers_->rmsnorm_layers_.at(layer_idx + config_->layer_num_);
    CHECK_NE(ffn_rmsnorm, nullptr)
        << "The final rmsnorm layer in the feedforward block is null pointer";
    STATUS_CHECK(ffn_rmsnorm->forward(input, ffn_norm_output));

    // w1
    tensor::Tensor w1_output = get_buffer(ModelBufferType::kW1Output);
    const auto& w1_layer = llama_layers_->w1_layers_.at(layer_idx);
    CHECK_NE(w1_layer, nullptr)
        << "The w1 layer in the feedforward block is null pointer";
    STATUS_CHECK(w1_layer->forward(ffn_norm_output, w1_output));

    // w3
    tensor::Tensor w3_ouput = get_buffer(ModelBufferType::kW3Output);
    const auto& w3_layer = llama_layers_->w3_layers_.at(layer_idx);
    CHECK_NE(w3_layer, nullptr)
        << "The w3 layer in the feedforward block is null pointer";
    STATUS_CHECK(w3_layer->forward(ffn_norm_output, w3_ouput));

    // SwiGLU
    CHECK_NE(llama_layers_->swiglu_layer_, nullptr)
        << "The swiglu layer in the feedforward block is null pointer";
    STATUS_CHECK(llama_layers_->swiglu_layer_->forward(w1_output, w3_ouput, w1_output));

    // w2
    tensor::Tensor w2_output = get_buffer(ModelBufferType::kW2Output);
    const auto& w2_layer = llama_layers_->w2_layers_.at(layer_idx);
    CHECK_NE(w2_layer, nullptr)
        << "The w2 layer in the feedforward block is null pointer";
    STATUS_CHECK(w2_layer->forward(w1_output, w2_output));

    // residual add
    CHECK_NE(llama_layers_->add_layer_, nullptr)
        << "The add layer in the feedforward block is null pointer";
    STATUS_CHECK(llama_layers_->add_layer_->forward(input, w2_output, input));
}

/**
 * @brief 计算最终分类 Logits
 * Norm -> Linear
 */
void LLamaModel::cls_logits(const tensor::Tensor& input) const {
    CHECK(llama_layers_ != nullptr);
    const auto& norm = llama_layers_->rmsnorm_layers_.at(2 * config_->layer_num_);
    CHECK_NE(norm, nullptr);
    STATUS_CHECK(norm->forward(input, input));

    tensor::Tensor forward_output = get_buffer(ModelBufferType::kForwardOutput);
    CHECK_NE(llama_layers_->cls_layer_, nullptr);
    STATUS_CHECK(llama_layers_->cls_layer_->forward(input, forward_output));
}

int32_t LLamaModel::post_processing(const tensor::Tensor& pos, bool is_prompt) const {
    tensor::Tensor forward_output = get_buffer(ModelBufferType::kForwardOutput);
    const float* forward_logits = forward_output.ptr<float>();

    int32_t next = 0;
    if (is_prompt) {
        next = -1;
    } else {
        next = static_cast<int32_t>(
            sampler_->sample(forward_logits, forward_output.size(),
                             cuda_config_ ? cuda_config_->stream : nullptr));
    }
    return next;
}

}  // namespace model
